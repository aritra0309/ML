{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":89659,"databundleVersionId":11735795,"sourceType":"competition"},{"sourceId":224423433,"sourceType":"kernelVersion"},{"sourceId":263093,"sourceType":"modelInstanceVersion","modelInstanceId":225001,"modelId":164716}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aritra423/drawing-with-llm-pali-gemma-2?scriptVersionId=241204845\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppresses INFO and WARNING logs\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow INFO and WARNING messages\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:32:39.963509Z","iopub.execute_input":"2025-05-22T10:32:39.964174Z","iopub.status.idle":"2025-05-22T10:32:39.970895Z","shell.execute_reply.started":"2025-05-22T10:32:39.964149Z","shell.execute_reply":"2025-05-22T10:32:39.970343Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install cairosvg\n!pip install clip\n!pip install torch\n!pip install bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:32:39.971751Z","iopub.execute_input":"2025-05-22T10:32:39.971921Z","iopub.status.idle":"2025-05-22T10:34:09.817862Z","shell.execute_reply.started":"2025-05-22T10:32:39.971906Z","shell.execute_reply":"2025-05-22T10:34:09.81711Z"}},"outputs":[{"name":"stdout","text":"Collecting cairosvg\n  Downloading cairosvg-2.8.2-py3-none-any.whl.metadata (2.7 kB)\nCollecting cairocffi (from cairosvg)\n  Downloading cairocffi-1.7.1-py3-none-any.whl.metadata (3.3 kB)\nCollecting cssselect2 (from cairosvg)\n  Downloading cssselect2-0.8.0-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from cairosvg) (0.7.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from cairosvg) (11.1.0)\nRequirement already satisfied: tinycss2 in /usr/local/lib/python3.11/dist-packages (from cairosvg) (1.4.0)\nRequirement already satisfied: cffi>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from cairocffi->cairosvg) (1.17.1)\nRequirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from cssselect2->cairosvg) (0.5.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.1.0->cairocffi->cairosvg) (2.22)\nDownloading cairosvg-2.8.2-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cairocffi-1.7.1-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cssselect2-0.8.0-py3-none-any.whl (15 kB)\nInstalling collected packages: cssselect2, cairocffi, cairosvg\nSuccessfully installed cairocffi-1.7.1 cairosvg-2.8.2 cssselect2-0.8.0\nCollecting clip\n  Downloading clip-0.2.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-0.2.0-py3-none-any.whl size=6989 sha256=51573eb53d24c604d6e94f9cf6cd817d0b2cf5e98eb9cec8e40d6c7291ee8b54\n  Stored in directory: /root/.cache/pip/wheels/ab/a5/e8/c9fa20742edbccf2702dae8ee62053e6c460e961d45967b49c\nSuccessfully built clip\nInstalling collected packages: clip\nSuccessfully installed clip-0.2.0\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.5\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Kaagle Metric\n","metadata":{}},{"cell_type":"code","source":"import ast\nimport io\nimport math\nimport statistics\nimport string\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFilter\nimport cairosvg\nimport clip\nimport torch\nimport torch.nn as nn\nimport cv2\nfrom more_itertools import chunked\n\nimport kagglehub\nsvg_constraints = kagglehub.package_import('metric/svg-constraints')\n\nclass ParticipantVisibleError(Exception):\n    pass\n\ndef score(\n    solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, random_seed: int = 0\n) -> float:\n    for colname in ['question', 'choices', 'answer']:\n        solution[colname] = solution[colname].apply(ast.literal_eval)\n    solution = solution.explode(['question', 'choices', 'answer'])\n\n    if not pd.api.types.is_string_dtype(submission.loc[:, 'svg']):\n        raise ParticipantVisibleError('svg must be a string.')\n\n    constraints = svg_constraints.SVGConstraints()\n    try:\n        for svg in submission.loc[:, 'svg']:\n            constraints.validate_svg(svg)\n    except Exception:\n        raise ParticipantVisibleError('SVG code violates constraints.')\n\n    vqa_evaluator = VQAEvaluator()\n    aesthetic_evaluator = AestheticEvaluator()\n\n    results = []\n    rng = np.random.RandomState(random_seed)\n    try:\n        df = solution.merge(submission, on='id')\n        for i, (_, group) in enumerate(df.loc[\n            :, ['id', 'question', 'choices', 'answer', 'svg']\n        ].groupby('id')):\n            questions, choices, answers, svg = [\n                group[col_name].to_list()\n                for col_name in group.drop('id', axis=1).columns\n            ]\n            svg = svg[0]\n            group_seed = rng.randint(0, np.iinfo(np.int32).max)\n            image_processor = ImageProcessor(image=svg_to_png(svg), seed=group_seed).apply()\n            image = image_processor.image.copy()\n            aesthetic_score = aesthetic_evaluator.score(image)\n            vqa_score = vqa_evaluator.score(questions, choices, answers, image)\n            image_processor.reset().apply_random_crop_resize().apply_jpeg_compression(quality=90)\n            ocr_score = vqa_evaluator.ocr(image_processor.image)\n            instance_score = (\n                harmonic_mean(vqa_score, aesthetic_score, beta=0.5) * ocr_score\n            )\n            results.append(instance_score)\n    except Exception:\n        raise ParticipantVisibleError('SVG failed to score.')\n\n    fidelity = statistics.mean(results)\n    return float(fidelity)\n\nclass VQAEvaluator:\n    def __init__(self):\n        from transformers import (\n            AutoProcessor,\n            BitsAndBytesConfig,\n            PaliGemmaForConditionalGeneration,\n        )\n        self.quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type='nf4',\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.float16,\n        )\n        self.letters = string.ascii_uppercase\n        self.model_path = kagglehub.model_download(\n            'google/paligemma-2/transformers/paligemma2-10b-mix-448'\n        )\n        self.processor = AutoProcessor.from_pretrained(self.model_path)\n        self.model = PaliGemmaForConditionalGeneration.from_pretrained(\n            self.model_path,\n            low_cpu_mem_usage=True,\n            quantization_config=self.quantization_config,\n        ).to('cuda:0')\n\n    def score(self, questions, choices, answers, image, n=4):\n        scores = []\n        batches = (chunked(qs, n) for qs in [questions, choices, answers])\n        for question_batch, choice_batch, answer_batch in zip(*batches, strict=True):\n            scores.extend(\n                self.score_batch(\n                    image,\n                    question_batch,\n                    choice_batch,\n                    answer_batch,\n                )\n            )\n        return statistics.mean(scores)\n\n    def score_batch(\n        self,\n        image: Image.Image,\n        questions: list[str],\n        choices_list: list[list[str]],\n        answers: list[str],\n    ) -> list[float]:\n        prompts = [\n            self.format_prompt(question, choices)\n            for question, choices in zip(questions, choices_list, strict=True)\n        ]\n        batched_choice_probabilities = self.get_choice_probability(\n            image, prompts, choices_list\n        )\n\n        scores = []\n        for i, _ in enumerate(questions):\n            choice_probabilities = batched_choice_probabilities[i]\n            answer = answers[i]\n            answer_probability = 0.0\n            for choice, prob in choice_probabilities.items():\n                if choice == answer:\n                    answer_probability = prob\n                    break\n            scores.append(answer_probability)\n\n        return scores\n\n    def format_prompt(self, question: str, choices: list[str]) -> str:\n        prompt = f'<image>answer en Question: {question}\\nChoices:\\n'\n        for i, choice in enumerate(choices):\n            prompt += f'{self.letters[i]}. {choice}\\n'\n        return prompt\n\n    def mask_choices(self, logits, choices_list):\n        batch_size = logits.shape[0]\n        masked_logits = torch.full_like(logits, float('-inf'))\n\n        for batch_idx in range(batch_size):\n            choices = choices_list[batch_idx]\n            for i in range(len(choices)):\n                letter_token = self.letters[i]\n                first_token = self.processor.tokenizer.encode(\n                    letter_token, add_special_tokens=False\n                )[0]\n                first_token_with_space = self.processor.tokenizer.encode(\n                    ' ' + letter_token, add_special_tokens=False\n                )[0]\n\n                if isinstance(first_token, int):\n                    masked_logits[batch_idx, first_token] = logits[\n                        batch_idx, first_token\n                    ]\n                if isinstance(first_token_with_space, int):\n                    masked_logits[batch_idx, first_token_with_space] = logits[\n                        batch_idx, first_token_with_space\n                    ]\n\n        return masked_logits\n\n    def get_choice_probability(self, image, prompts, choices_list) -> list[dict]:\n        inputs = self.processor(\n            images=[image] * len(prompts),\n            text=prompts,\n            return_tensors='pt',\n            padding='longest',\n        ).to('cuda:0')\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            logits = outputs.logits[:, -1, :]\n            masked_logits = self.mask_choices(logits, choices_list)\n            probabilities = torch.softmax(masked_logits, dim=-1)\n\n        batched_choice_probabilities = []\n        for batch_idx in range(len(prompts)):\n            choice_probabilities = {}\n            choices = choices_list[batch_idx]\n            for i, choice in enumerate(choices):\n                letter_token = self.letters[i]\n                first_token = self.processor.tokenizer.encode(\n                    letter_token, add_special_tokens=False\n                )[0]\n                first_token_with_space = self.processor.tokenizer.encode(\n                    ' ' + letter_token, add_special_tokens=False\n                )[0]\n\n                prob = 0.0\n                if isinstance(first_token, int):\n                    prob += probabilities[batch_idx, first_token].item()\n                if isinstance(first_token_with_space, int):\n                    prob += probabilities[batch_idx, first_token_with_space].item()\n                choice_probabilities[choice] = prob\n\n            total_prob = sum(choice_probabilities.values())\n            if total_prob > 0:\n                renormalized_probabilities = {\n                    choice: prob / total_prob\n                    for choice, prob in choice_probabilities.items()\n                }\n            else:\n                renormalized_probabilities = choice_probabilities\n            batched_choice_probabilities.append(renormalized_probabilities)\n\n        return batched_choice_probabilities\n\n    def ocr(self, image, free_chars=4):\n        inputs = (\n            self.processor(\n                text='<image>ocr\\n',\n                images=image,\n                return_tensors='pt',\n            )\n            .to(torch.float16)\n            .to(self.model.device)\n        )\n        input_len = inputs['input_ids'].shape[-1]\n\n        with torch.inference_mode():\n            outputs = self.model.generate(**inputs, max_new_tokens=32, do_sample=False)\n            outputs = outputs[0][input_len:]\n            decoded = self.processor.decode(outputs, skip_special_tokens=True)\n\n        num_char = len(decoded)\n        return min(1.0, math.exp(-num_char + free_chars))\n\nclass AestheticPredictor(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.input_size = input_size\n        self.layers = nn.Sequential(\n            nn.Linear(self.input_size, 1024),\n            nn.Dropout(0.2),\n            nn.Linear(1024, 128),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.Dropout(0.1),\n            nn.Linear(64, 16),\n            nn.Linear(16, 1),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nclass AestheticEvaluator:\n    def __init__(self):\n        self.model_path = '/kaggle/input/sac-logos-ava1-l14-linearmse/sac+logos+ava1-l14-linearMSE.pth'\n        self.clip_model_path = '/kaggle/input/openai-clip-vit-large-patch14/ViT-L-14.pt'\n        self.predictor, self.clip_model, self.preprocessor = self.load()\n\n    def load(self):\n        state_dict = torch.load(self.model_path, weights_only=True, map_location='cuda:1')\n        predictor = AestheticPredictor(768)\n        predictor.load_state_dict(state_dict)\n        predictor.to('cuda:1')\n        predictor.eval()\n        clip_model, preprocessor = clip.load(self.clip_model_path, device='cuda:1')\n        return predictor, clip_model, preprocessor\n\n    def score(self, image: Image.Image) -> float:\n        image = self.preprocessor(image).unsqueeze(0).to('cuda:1')\n        with torch.no_grad():\n            image_features = self.clip_model.encode_image(image)\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n            image_features = image_features.cpu().detach().numpy()\n        score = self.predictor(torch.from_numpy(image_features).to('cuda:1').float())\n        return score.item() / 10.0\n\ndef harmonic_mean(a: float, b: float, beta: float = 1.0) -> float:\n    if a <= 0 or b <= 0:\n        return 0.0\n    return (1 + beta**2) * (a * b) / (beta**2 * a + b)\n\ndef svg_to_png(svg_code: str, size: tuple = (384, 384)) -> Image.Image:\n    if 'viewBox' not in svg_code:\n        svg_code = svg_code.replace('<svg', f'<svg viewBox=\"0 0 {size[0]} {size[1]}\"')\n    png_data = cairosvg.svg2png(bytestring=svg_code.encode('utf-8'))\n    return Image.open(io.BytesIO(png_data)).convert('RGB').resize(size)\n\nclass ImageProcessor:\n    def __init__(self, image: Image.Image, seed=None):\n        self.image = image\n        self.original_image = self.image.copy()\n        if seed is not None:\n            self.rng = np.random.RandomState(seed)\n        else:\n            self.rng = np.random\n\n    def reset(self):\n        self.image = self.original_image.copy()\n        return self\n\n    def apply_median_filter(self, size=3):\n        self.image = self.image.filter(ImageFilter.MedianFilter(size=size))\n        return self\n\n    def apply_bilateral_filter(self, d=9, sigma_color=75, sigma_space=75):\n        img_array = np.asarray(self.image)\n        filtered = cv2.bilateralFilter(img_array, d, sigma_color, sigma_space)\n        self.image = Image.fromarray(filtered)\n        return self\n\n    def apply_fft_low_pass(self, cutoff_frequency=0.5):\n        img_array = np.array(self.image, dtype=np.float32)\n        result = np.zeros_like(img_array)\n        for i in range(3):\n            f = np.fft.fft2(img_array[:, :, i])\n            fshift = np.fft.fftshift(f)\n            rows, cols = img_array[:, :, i].shape\n            crow, ccol = rows // 2, cols // 2\n            mask = np.zeros((rows, cols), np.float32)\n            r = int(min(crow, ccol) * cutoff_frequency)\n            center = [crow, ccol]\n            x, y = np.ogrid[:rows, :cols]\n            mask_area = (x - center[0]) ** 2 + (y - center[1]) ** 2 <= r * r\n            mask[mask_area] = 1\n            fshift_filtered = fshift * mask\n            f_ishift = np.fft.ifftshift(fshift_filtered)\n            img_back = np.fft.ifft2(f_ishift)\n            img_back = np.real(img_back)\n            result[:, :, i] = img_back\n        result = np.clip(result, 0, 255).astype(np.uint8)\n        self.image = Image.fromarray(result)\n        return self\n\n    def apply_jpeg_compression(self, quality=85):\n        buffer = io.BytesIO()\n        self.image.save(buffer, format='JPEG', quality=quality)\n        buffer.seek(0)\n        self.image = Image.open(buffer)\n        return self\n\n    def apply_random_crop_resize(self, crop_percent=0.05):\n        width, height = self.image.size\n        crop_pixels_w = int(width * crop_percent)\n        crop_pixels_h = int(height * crop_percent)\n        left = self.rng.randint(0, crop_pixels_w + 1)\n        top = self.rng.randint(0, crop_pixels_h + 1)\n        right = width - self.rng.randint(0, crop_pixels_w + 1)\n        bottom = height - self.rng.randint(0, crop_pixels_h + 1)\n        self.image = self.image.crop((left, top, right, bottom))\n        self.image = self.image.resize((width, height), Image.BILINEAR)\n        return self\n\n    def apply(self):\n        return (\n            self.apply_random_crop_resize(crop_percent=0.03)\n            .apply_jpeg_compression(quality=95)\n            .apply_median_filter(size=9)\n            .apply_fft_low_pass(cutoff_frequency=0.5)\n            .apply_bilateral_filter(d=5, sigma_color=75, sigma_space=75)\n            .apply_jpeg_compression(quality=92)\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:34:09.818916Z","iopub.execute_input":"2025-05-22T10:34:09.819143Z","iopub.status.idle":"2025-05-22T10:34:15.992434Z","shell.execute_reply.started":"2025-05-22T10:34:09.819119Z","shell.execute_reply":"2025-05-22T10:34:15.991862Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig, PaliGemmaForConditionalGeneration\nimport kagglehub\nimport os\n\nprint(\"Using device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Print the installed transformers version\nimport transformers\nprint(f\"Transformers version: {transformers.__version__}\")\n\n# Download model files\nmodel_path = kagglehub.model_download('google/paligemma-2/transformers/paligemma2-10b-mix-448')\nprint(f\"Model path: {model_path}\")\nprint(f\"Files in model directory: {os.listdir(model_path)}\")\n\n# Try finding spiece.model, tokenizer.model or similar files\nmodel_files = []\nfor root, dirs, files in os.walk(os.path.dirname(model_path)):\n    for file in files:\n        if file.endswith('.model'):\n            model_files.append(os.path.join(root, file))\nprint(f\"Model files found: {model_files}\")\n\n# Configure quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\n# Try loading directly with the model path\ntry:\n    print(\"Attempting to load model directly...\")\n    model = PaliGemmaForConditionalGeneration.from_pretrained(\n        model_path,\n        quantization_config=quantization_config,\n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True,\n        device_map=\"auto\",\n    )\n    print(\"Model loaded successfully!\")\n    \n    # Try loading processor directly\n    try:\n        processor = AutoProcessor.from_pretrained(model_path)\n        print(\"Processor loaded successfully!\")\n    except Exception as e:\n        print(f\"Error loading processor: {e}\")\n        # If that fails, try a different approach (using your earlier working code)\n        from transformers import PreTrainedTokenizerFast\n        tokenizer = PreTrainedTokenizerFast(tokenizer_file=os.path.join(model_path, \"tokenizer.json\"))\n        print(\"Loaded tokenizer using PreTrainedTokenizerFast\")\n        \nexcept Exception as e:\n    print(f\"Error loading model: {e}\")\n    print(\"Attempting alternative loading method...\")\n\nprint(\"PaLiGemma 2 loading process complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:34:15.994473Z","iopub.execute_input":"2025-05-22T10:34:15.994864Z","iopub.status.idle":"2025-05-22T10:39:06.520759Z","shell.execute_reply.started":"2025-05-22T10:34:15.994845Z","shell.execute_reply":"2025-05-22T10:39:06.520087Z"}},"outputs":[{"name":"stderr","text":"2025-05-22 10:34:24.064128: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747910064.261830      59 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747910064.314075      59 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\nTransformers version: 4.51.3\nModel path: /kaggle/input/paligemma-2/transformers/paligemma2-10b-mix-448/1\nFiles in model directory: ['model.safetensors.index.json', 'model-00003-of-00004.safetensors', 'config.json', 'preprocessor_config.json', 'model-00001-of-00004.safetensors', 'tokenizer.json', 'tokenizer_config.json', 'model-00004-of-00004.safetensors', 'special_tokens_map.json', 'model-00002-of-00004.safetensors', 'generation_config.json']\nModel files found: []\nAttempting to load model directly...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76bfc0ecb89a40babb8313866f5c00cb"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"Model loaded successfully!\nProcessor loaded successfully!\nPaLiGemma 2 loading process complete.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from diffusers import StableDiffusionPipeline\nimport torch\n\ndef load_pipe_auto_device(model_name=\"CompVis/stable-diffusion-v1-4\"):\n    try:\n        if torch.cuda.is_available():\n            print(\"Trying to load pipeline on GPU...\")\n            pipe = StableDiffusionPipeline.from_pretrained(model_name)\n            pipe = pipe.to(\"cuda\")\n            _ = pipe(\"a photo of a cat\", num_inference_steps=1)\n            print(\"Pipeline loaded on GPU.\")\n        else:\n            raise RuntimeError(\"CUDA not available\")\n    except Exception as e:\n        print(f\"Falling back to CPU due to: {e}\")\n        pipe = StableDiffusionPipeline.from_pretrained(model_name)\n        pipe = pipe.to(\"cpu\")\n        print(\"Pipeline loaded on CPU.\")\n    return pipe\n\n# Usage:\npipe = load_pipe_auto_device()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:39:06.521513Z","iopub.execute_input":"2025-05-22T10:39:06.522216Z","iopub.status.idle":"2025-05-22T10:39:47.017669Z","shell.execute_reply.started":"2025-05-22T10:39:06.522194Z","shell.execute_reply":"2025-05-22T10:39:47.016984Z"}},"outputs":[{"name":"stdout","text":"Trying to load pipeline on GPU...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd910c2c4475480892bdb71bd54a9b23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c899195d65b545ac84a736e24560a274"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbabe80f4e284b5f8abbd80673d10f84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a830d465f3c4fb0b8e295b9d8bb10be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afd7630239ed464aac91607a2f36b508"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbc21bc718ed4a1290f0720823e7a99e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler_config-checkpoint.json:   0%|          | 0.00/209 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5dfd3251d78445ea8cad45c17c4f223"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler_config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33f2b280a8ba466a864120973c7ff918"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"text_encoder/model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43da2d51d9414f6a8f7d8eb7165d9ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"safety_checker/model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1072190acb164d9d94a9148c50f601a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfa206e24b5640dca1a983d9bc07259b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e4d05d2873a4d3998581603dfb876b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f12597917614d92808c2f06f6fd48e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unet/diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5da46c35f53b4313bd17fb5f3ec874c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89c2dfcadd2b4c3986c1f730ab3fd232"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef36882794374849b1487edaf1de511e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8020b4401072418f8acc52eca6846c58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24a3340cb52d41d190795c63a5ba44f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fecbfb203ed4a4d9eab541f3e84917e"}},"metadata":{}},{"name":"stderr","text":"Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n","output_type":"stream"},{"name":"stdout","text":"Pipeline loaded on GPU.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom PIL import Image\nfrom skimage.segmentation import slic\nfrom skimage.measure import regionprops, label\nfrom skimage.color import rgb2lab, rgb2gray\nfrom skimage.filters import sobel\n\ndef compress_hex_color(hex_color):\n    r, g, b = int(hex_color[1:3], 16), int(hex_color[3:5], 16), int(hex_color[5:7], 16)\n    if r % 17 == 0 and g % 17 == 0 and b % 17 == 0:\n        return f'#{r//17:x}{g//17:x}{b//17:x}'\n    return hex_color\n\ndef fit_polygon(contour, max_error=2.0):\n    \"\"\"Fit a polygon to the given contour using the Ramer-Douglas-Peucker algorithm.\"\"\"\n    epsilon = max_error\n    approx = cv2.approxPolyDP(contour, epsilon, True)\n    return approx\n\ndef fit_ellipse(contour):\n    \"\"\"Fit an ellipse to the given contour if possible.\"\"\"\n    if len(contour) < 5:\n        return None\n    ellipse = cv2.fitEllipse(contour)\n    return ellipse\n\ndef color_contrast(color1, color2):\n    # Use Euclidean distance in LAB color space for perceptual contrast\n    l1 = rgb2lab(np.array([[color1]], dtype=np.uint8)/255.0)[0,0]\n    l2 = rgb2lab(np.array([[color2]], dtype=np.uint8)/255.0)[0,0]\n    return np.linalg.norm(l1 - l2)\n\ndef extract_advanced_features(img_np, max_superpixels=200, num_colors=16):\n    \"\"\"Multi-scale segmentation, edge detection, and saliency-weighted feature extraction.\"\"\"\n    # Ensure RGB\n    if img_np.shape[2] == 4:\n        img_np = img_np[:, :, :3]\n    img_rgb = img_np if img_np.shape[2] == 3 else cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)\n    \n    height, width = img_rgb.shape[:2]\n    img_lab = rgb2lab(img_rgb / 255.0)\n    img_gray = rgb2gray(img_rgb)\n    edge_map = sobel(img_gray)\n    \n    # SLIC superpixel segmentation for multi-scale features\n    segments = slic(img_rgb, n_segments=max_superpixels, compactness=10, start_label=1)\n    region_labels = label(segments)\n    regions = regionprops(region_labels)\n\n    # K-means color quantization (better error handling)\n    pixels = img_rgb.reshape(-1, 3).astype(np.float32)\n    unique_colors = np.unique(pixels, axis=0)\n    k = min(len(unique_colors), num_colors)\n    if k < 2:\n        # Fallback: image is almost uniform\n        quantized = img_rgb.copy()\n        palette = np.array([img_rgb.reshape(-1, 3)[0]])\n    else:\n        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n        _, labels, centers = cv2.kmeans(pixels, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n        palette = centers.astype(np.uint8)\n        quantized = palette[labels.flatten()].reshape(img_rgb.shape)\n\n    # Saliency: edge strength, area, color contrast, and spatial centrality\n    center_x, center_y = width / 2, height / 2\n    features = []\n    min_area = 10  # Lowered for small details\n\n    for region in regions:\n        minr, minc, maxr, maxc = region.bbox\n        mask = (region_labels == region.label).astype(np.uint8)\n        contour_list, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if len(contour_list) == 0:\n            continue\n        contour = max(contour_list, key=cv2.contourArea)\n        area = cv2.contourArea(contour)\n        if area < min_area:\n            continue\n        # Dominant color in this region (mode of quantized image in region)\n        region_pixels = quantized[mask.astype(bool)]\n        if len(region_pixels) == 0:\n            continue\n        unique, counts = np.unique(region_pixels.reshape(-1, 3), axis=0, return_counts=True)\n        dom_color = unique[counts.argmax()]\n        hex_color = compress_hex_color(f'#{dom_color[0]:02x}{dom_color[1]:02x}{dom_color[2]:02x}')\n        \n        # Fit a polygon (could also try ellipse or Bezier)\n        poly = fit_polygon(contour, max_error=3.0)\n        if len(poly) < 3:\n            continue\n        points = \" \".join([f\"{pt[0][0]:.2f},{pt[0][1]:.2f}\" for pt in poly])\n        \n        # Optionally fit ellipse if region is roughly elliptical\n        ellipse_svg = \"\"\n        ellipse = fit_ellipse(contour)\n        if ellipse is not None and area / (np.pi * ellipse[1][0] / 2 * ellipse[1][1] / 2) > 0.85:\n            (cx, cy), (maj, min), angle = ellipse\n            ellipse_svg = f'<ellipse cx=\"{cx:.2f}\" cy=\"{cy:.2f}\" rx=\"{maj/2:.2f}\" ry=\"{min/2:.2f}\" fill=\"{hex_color}\" transform=\"rotate({angle:.1f} {cx:.2f} {cy:.2f})\"/>'\n        \n        # Importance: area, edge saliency, color contrast to mean bg, centrality\n        m = cv2.moments(contour)\n        cx = int(m[\"m10\"] / m[\"m00\"]) if m[\"m00\"] > 0 else (minc + maxc) / 2\n        cy = int(m[\"m01\"] / m[\"m00\"]) if m[\"m00\"] > 0 else (minr + maxr) / 2\n        dist_from_center = np.sqrt(((cx - center_x) / width) ** 2 + ((cy - center_y) / height) ** 2)\n        mean_edge = np.mean(edge_map[mask.astype(bool)])\n        bg_color = np.mean(img_rgb, axis=(0, 1)).astype(int)\n        contrast = color_contrast(dom_color, bg_color)\n        importance = (area * (1 - dist_from_center) * (mean_edge + 0.2) * (contrast + 5))\n        features.append({\n            'points': points,\n            'color': hex_color,\n            'area': area,\n            'importance': importance,\n            'ellipse_svg': ellipse_svg\n        })\n    features.sort(key=lambda x: x['importance'], reverse=True)\n\n    # Fallback: add a full-rectangle feature if nothing detected\n    if len(features) == 0:\n        dom_color = np.mean(img_rgb, axis=(0, 1)).astype(int)\n        hex_color = compress_hex_color(f'#{dom_color[0]:02x}{dom_color[1]:02x}{dom_color[2]:02x}')\n        features.append({\n            'points': f\"0,0 {width},0 {width},{height} 0,{height}\",\n            'color': hex_color,\n            'area': width * height,\n            'importance': 1.0,\n            'ellipse_svg': \"\"\n        })\n    return features\n\ndef advanced_bitmap_to_svg(\n    image,\n    max_size_bytes=20000,\n    resize=True,\n    target_size=(384, 384),\n    max_superpixels=200,\n    num_colors=16,\n    use_ellipses=True,\n    group_similar=True,\n):\n    \"\"\"\n    Convert a bitmap PIL image to an advanced, high-fidelity SVG string.\n    \"\"\"\n    if resize:\n        original_size = image.size\n        image = image.resize(target_size, Image.LANCZOS)\n    else:\n        original_size = image.size\n    img_np = np.array(image)\n    height, width = img_np.shape[:2]\n    avg_bg_color = np.mean(img_np[:,:,:3], axis=(0,1)).astype(int) if img_np.shape[2] >= 3 else 255\n    bg_hex_color = compress_hex_color(f'#{avg_bg_color[0]:02x}{avg_bg_color[1]:02x}{avg_bg_color[2]:02x}')\n    orig_width, orig_height = original_size\n    svg = [f'<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"{orig_width}\" height=\"{orig_height}\" viewBox=\"0 0 {width} {height}\">']\n    svg.append(f'<rect width=\"{width}\" height=\"{height}\" fill=\"{bg_hex_color}\"/>')\n    features = extract_advanced_features(\n        img_np,\n        max_superpixels=max_superpixels,\n        num_colors=num_colors\n    )\n    # Optionally group polygons by color for smaller SVG\n    if group_similar:\n        from collections import defaultdict\n        color_groups = defaultdict(list)\n        for f in features:\n            color_groups[f['color']].append(f)\n        for color, feats in sorted(color_groups.items(), key=lambda x: -sum(f['importance'] for f in x[1])):\n            svg.append(f'<g fill=\"{color}\">')\n            for feat in feats:\n                if use_ellipses and feat['ellipse_svg']:\n                    svg.append(feat['ellipse_svg'])\n                svg.append(f'<polygon points=\"{feat[\"points\"]}\"/>')\n            svg.append('</g>')\n    else:\n        for feat in features:\n            if use_ellipses and feat['ellipse_svg']:\n                svg.append(feat['ellipse_svg'])\n            svg.append(f'<polygon points=\"{feat[\"points\"]}\" fill=\"{feat[\"color\"]}\"/>')\n    svg.append('</svg>')\n    svg_str = \"\".join(svg)\n    # Truncate if needed\n    if len(svg_str.encode('utf-8')) > max_size_bytes:\n        svg_str = f'<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"{orig_width}\" height=\"{orig_height}\" viewBox=\"0 0 {width} {height}\"><rect width=\"{width}\" height=\"{height}\" fill=\"{bg_hex_color}\"/></svg>'\n    return svg_str","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:39:47.01851Z","iopub.execute_input":"2025-05-22T10:39:47.018809Z","iopub.status.idle":"2025-05-22T10:39:47.283958Z","shell.execute_reply.started":"2025-05-22T10:39:47.01879Z","shell.execute_reply":"2025-05-22T10:39:47.283379Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#| export\n\ndef generate_bitmap_advanced(\n    prompt,\n    negative_prompt=\"\",\n    num_inference_steps=20,\n    guidance_scale=15,\n    pipe=None,\n    device=None,\n    verbose=False,\n    seed=None,\n    **extra_kwargs\n):\n    \"\"\"\n    Generate an image using a diffusion pipeline with advanced options.\n    \n    Parameters:\n        prompt (str): The prompt for image generation.\n        negative_prompt (str): Features to avoid.\n        num_inference_steps (int): Number of diffusion steps.\n        guidance_scale (float): Classifier-free guidance scale.\n        pipe: Diffusion pipeline object (if None, uses global 'pipe').\n        device: Device for inference (if not None, moves pipe to device).\n        verbose (bool): If True, prints diagnostic information.\n        seed (int or None): Optional random seed for reproducibility.\n        extra_kwargs: Any additional keyword arguments for the pipeline.\n        \n    Returns:\n        image: The generated image (PIL.Image or similar).\n    \"\"\"\n    import torch\n\n    if pipe is None:\n        try:\n            pipe = globals()[\"pipe\"]\n        except KeyError:\n            raise ValueError(\"No 'pipe' diffusion pipeline provided or defined globally.\")\n\n    if device is not None:\n        pipe = pipe.to(device)\n        if verbose:\n            print(f\"Pipeline moved to device: {device}\")\n\n    generator = None\n    if seed is not None:\n        generator = torch.Generator(device=device if device else \"cpu\").manual_seed(seed)\n        if verbose:\n            print(f\"Using random seed: {seed}\")\n\n    if verbose:\n        print(f\"Prompt: {prompt}\")\n        print(f\"Negative prompt: {negative_prompt}\")\n        print(f\"Inference steps: {num_inference_steps}\")\n        print(f\"Guidance scale: {guidance_scale}\")\n\n    result = pipe(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        generator=generator,\n        **extra_kwargs\n    )\n    image = result.images[0]\n    if verbose:\n        print(\"Image generated successfully.\")\n    return image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:39:47.28478Z","iopub.execute_input":"2025-05-22T10:39:47.285404Z","iopub.status.idle":"2025-05-22T10:39:50.685814Z","shell.execute_reply.started":"2025-05-22T10:39:47.285374Z","shell.execute_reply":"2025-05-22T10:39:50.684878Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import threading\n\n# Global model objects for single initialization (thread-safe)\n_global_vqa_evaluator = None\n_global_aesthetic_evaluator = None\n_evaluator_lock = threading.Lock()\n\ndef initialize_evaluators():\n    \"\"\"Thread-safe, initialize and cache evaluators.\"\"\"\n    global _global_vqa_evaluator, _global_aesthetic_evaluator\n    with _evaluator_lock:\n        if _global_vqa_evaluator is None:\n            print(\"[Evaluator] Initializing VQA Evaluator...\")\n            _global_vqa_evaluator = VQAEvaluator()\n        if _global_aesthetic_evaluator is None:\n            print(\"[Evaluator] Initializing Aesthetic Evaluator...\")\n            _global_aesthetic_evaluator = AestheticEvaluator()\n    return _global_vqa_evaluator, _global_aesthetic_evaluator\n\ndef evaluate_svg(svg: str, prompt: str, \n                 vqa_weight: float = 2.0, \n                 cache: dict = None, \n                 complexity_alpha: float = 0.015,\n                 verbose: bool = False) -> dict:\n    \"\"\"\n    Evaluate SVG illustration quality for a competition:\n      - VQA (semantic relevance to prompt)\n      - Aesthetics (CLIP-based)\n      - SVG complexity penalty (optional)\n    Returns a dict of scores.\n    \"\"\"\n    cache_key = (svg, prompt)\n    if cache is not None and cache_key in cache:\n        return cache[cache_key]\n\n    vqa_evaluator, aesthetic_evaluator = initialize_evaluators()\n    image = svg_to_png(svg)\n    \n    vqa_prompt = f\"SVG illustration of {prompt}\"\n    vqa_score = vqa_evaluator.score(image, vqa_prompt)\n    aesthetic_score = aesthetic_evaluator.score(image)\n    complexity_penalty = svg_complexity_penalty(svg, alpha=complexity_alpha)\n\n    combined_score = harmonic_mean(vqa_score, aesthetic_score, beta=vqa_weight) * complexity_penalty\n\n    result = {\n        'vqa_score': vqa_score,\n        'aesthetic_score': aesthetic_score,\n        'complexity_penalty': complexity_penalty,\n        'combined_score': combined_score\n    }\n    if cache is not None:\n        cache[cache_key] = result\n    if verbose:\n        print(f\"[Evaluator] Prompt: {prompt}\\n\"\n              f\"  VQA: {vqa_score:.3f}, Aesthetic: {aesthetic_score:.3f}, \"\n              f\"Complexity Penalty: {complexity_penalty:.3f}, \"\n              f\"Combined: {combined_score:.3f}\")\n    return result\n\ndef svg_complexity_penalty(svg_code: str, alpha: float = 0.015) -> float:\n    \"\"\"\n    Penalize SVGs with high command count to encourage simplicity.\n    \"\"\"\n    import re\n    num_commands = len(re.findall(r'([MLCQZmlcqz])', svg_code))\n    return float(np.exp(-alpha * num_commands))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:39:50.686815Z","iopub.execute_input":"2025-05-22T10:39:50.687126Z","iopub.status.idle":"2025-05-22T10:39:51.879913Z","shell.execute_reply.started":"2025-05-22T10:39:50.687101Z","shell.execute_reply":"2025-05-22T10:39:51.878862Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"default_svgs = [\n    # Advanced geometric + abstract SVG\n    '''\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"384\" height=\"384\">\n      <rect width=\"100%\" height=\"100%\" fill=\"#f5f5f5\"/>\n      <g stroke=\"#262626\" stroke-width=\"2\">\n        <rect x=\"40\" y=\"40\" width=\"304\" height=\"304\" rx=\"32\" fill=\"#e0eafc\"/>\n        <circle cx=\"192\" cy=\"192\" r=\"108\" fill=\"#aad8d3\" stroke=\"#2e3440\" stroke-width=\"8\"/>\n        <polygon points=\"192,70 250,314 134,314\" fill=\"#ef476f\" opacity=\"0.85\" stroke=\"#2e3440\" stroke-width=\"2\"/>\n        <polyline points=\"96,160 288,160 192,320 96,160\" fill=\"none\" stroke=\"#0077b6\" stroke-width=\"6\" opacity=\"0.7\"/>\n        <path d=\"M130 110 Q192 40 254 110 T254 274 Q192 344 130 274 T130 110\" fill=\"none\" stroke=\"#ff006e\" stroke-width=\"4\" opacity=\"0.8\"/>\n      </g>\n      <g fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n        <path stroke=\"#FF4700\" stroke-width=\"13\" d=\"M160 220q140-10 55-93\"/>\n        <path stroke=\"#FF5E00\" stroke-width=\"6\" d=\"M88 83q-14 90 170 74\"/>\n        <path stroke=\"#FDA1F6\" stroke-opacity=\".5\" stroke-width=\"11\" d=\"M280 175q23 27-200 100\"/>\n        <path stroke=\"#C77145\" stroke-opacity=\".7\" stroke-width=\"19\" d=\"M133 256q132 0 120-87\"/>\n      </g>\n    </svg>\n    ''',\n\n    # Layered SVG with ellipses and gradients\n    '''\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"384\" height=\"384\">\n      <defs>\n        <radialGradient id=\"bg\" cx=\"50%\" cy=\"50%\" r=\"65%\">\n          <stop offset=\"0%\" stop-color=\"#fffbe7\"/>\n          <stop offset=\"100%\" stop-color=\"#dbeafe\"/>\n        </radialGradient>\n      </defs>\n      <rect width=\"384\" height=\"384\" fill=\"url(#bg)\"/>\n      <ellipse cx=\"192\" cy=\"192\" rx=\"140\" ry=\"80\" fill=\"#a3be8c\" fill-opacity=\"0.7\"/>\n      <ellipse cx=\"120\" cy=\"192\" rx=\"60\" ry=\"140\" fill=\"#5e81ac\" fill-opacity=\"0.6\"/>\n      <ellipse cx=\"264\" cy=\"192\" rx=\"60\" ry=\"140\" fill=\"#bf616a\" fill-opacity=\"0.5\"/>\n      <circle cx=\"192\" cy=\"192\" r=\"48\" fill=\"#ebcb8b\"/>\n      <g stroke=\"#495867\" stroke-width=\"4\" fill=\"none\">\n        <path d=\"M140 120 Q192 40 244 120\"/>\n        <path d=\"M244 264 Q192 344 140 264\"/>\n        <path d=\"M80 192 Q192 80 304 192\"/>\n        <path d=\"M304 192 Q192 304 80 192\"/>\n      </g>\n    </svg>\n    ''',\n\n    # Abstract paths and corners\n    '''\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"384\" height=\"384\">\n      <rect width=\"384\" height=\"384\" fill=\"#fafafa\"/>\n      <rect x=\"0\" y=\"0\" width=\"384\" height=\"48\" fill=\"#b4c9e7\"/>\n      <rect x=\"0\" y=\"336\" width=\"384\" height=\"48\" fill=\"#b4c9e7\"/>\n      <rect x=\"0\" y=\"0\" width=\"48\" height=\"384\" fill=\"#b4c9e7\"/>\n      <rect x=\"336\" y=\"0\" width=\"48\" height=\"384\" fill=\"#b4c9e7\"/>\n      <g fill=\"none\" stroke=\"#4361ee\" stroke-width=\"10\" opacity=\"0.7\">\n        <path d=\"M96 96 Q192 32 288 96\"/>\n        <path d=\"M288 288 Q192 352 96 288\"/>\n        <polyline points=\"96,96 192,192 288,288\"/>\n        <polyline points=\"288,96 192,192 96,288\"/>\n      </g>\n      <g>\n        <circle cx=\"192\" cy=\"192\" r=\"40\" fill=\"#fcbf49\" stroke=\"#f77f00\" stroke-width=\"6\"/>\n      </g>\n    </svg>\n    ''',\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:39:51.880919Z","iopub.execute_input":"2025-05-22T10:39:51.881186Z","iopub.status.idle":"2025-05-22T10:39:52.932038Z","shell.execute_reply.started":"2025-05-22T10:39:51.881163Z","shell.execute_reply":"2025-05-22T10:39:52.931238Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"#| export\n\ndef get_best_default_svg(description, verbose=True):\n    \"\"\"\n    Select the best scoring SVG from the default_svgs list for a given description.\n    Uses your competition metric to score each SVG.\n    Returns (svg_string, score).\n    \"\"\"\n    global default_svgs\n\n    best_svg = None\n    best_score = float('-inf')\n\n    for idx, svg in enumerate(default_svgs):\n        try:\n            scores = evaluate_svg(svg, description)\n            combined_score = scores.get(\"combined_score\", 0)\n            if verbose:\n                print(f\"[Default SVG {idx+1}/{len(default_svgs)}] Score: {combined_score:.4f}\")\n        except Exception as e:\n            if verbose:\n                print(f\"[Default SVG {idx+1}/{len(default_svgs)}] Scoring error: {e}\")\n            continue\n\n        if combined_score > best_score:\n            best_svg = svg\n            best_score = combined_score\n\n    if verbose:\n        print(f\"Best default SVG score: {best_score:.4f}\")\n\n    return best_svg, best_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:39:52.932885Z","iopub.execute_input":"2025-05-22T10:39:52.933106Z","iopub.status.idle":"2025-05-22T10:39:53.968312Z","shell.execute_reply.started":"2025-05-22T10:39:52.933089Z","shell.execute_reply":"2025-05-22T10:39:53.967236Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nimport os\nimport time\n\n\ndef generate_and_convert_advanced(\n    prompt_prefix=\"\",\n    description=\"\",\n    prompt_suffix=\"\",\n    negative_prompt=\"\",\n    num_attempts=3,\n    num_inference_steps=20,\n    guidance_scale=15,\n    verbose=True,\n    fallback_svg='<svg width=\"200\" height=\"200\"></svg>',\n    cache=None,\n    postprocess_svg=None,\n    save_attempt_hook=None,\n    display_images=True,\n    use_best_default=True\n):\n    \"\"\"\n    Advanced pipeline for generating, converting, and evaluating SVGs for competition:\n      - Attempts multiple generations, tracks and returns the best-scoring result.\n      - Optionally uses the best default SVG as a baseline.\n      - Supports caching, hooks, and optional postprocessing.\n      - Prints detailed diagnostics and timing.\n    \"\"\"\n\n    prompt = prompt_prefix + \" \" + description + \" \" + prompt_suffix\n    total_start_time = time.time()\n    attempt_data = []\n\n    # 1. Baseline: Best default SVG\n    best_svg, best_score = None, float('-inf')\n    if use_best_default:\n        try:\n            default_svg, default_score = get_best_default_svg(description, verbose=verbose)\n            if verbose:\n                print(f\"\\n[Baseline] Best default SVG score: {default_score:.4f}\")\n            best_svg = default_svg\n            best_score = default_score\n            attempt_data.append({\n                'svg': default_svg,\n                'score': default_score,\n                'type': 'default'\n            })\n        except Exception as e:\n            if verbose:\n                print(\"[Baseline] Default SVG selection error:\", e)\n            best_svg = fallback_svg\n            best_score = 0.0\n\n    # 2. Generation Loop\n    timing = {\n        'generation': [],\n        'conversion': [],\n        'evaluation': [],\n        'postprocess': [],\n        'attempt': []\n    }\n\n    for i in range(num_attempts):\n        if verbose: print(f\"\\n=== Generation Attempt {i+1}/{num_attempts} ===\")\n        attempt_start = time.time()\n\n        # --- Generation ---\n        for gen_attempt in range(3):  # Try up to 3 times for each attempt\n            try:\n                torch.cuda.empty_cache()\n                torch.cuda.ipc_collect()\n                t0 = time.time()\n                with torch.no_grad():\n                    bitmap = generate_bitmap_advanced(\n                        prompt,\n                        negative_prompt=negative_prompt,\n                        num_inference_steps=num_inference_steps,\n                        guidance_scale=guidance_scale\n                    )\n                t1 = time.time()\n                timing['generation'].append(t1 - t0)\n                if verbose: print(f\"Bitmap generation: {t1-t0:.2f}s\")\n                break  # Successful, break retry loop\n            except RuntimeError as e:\n                if \"CUDA out of memory\" in str(e):\n                    print(f\"[Attempt {i+1}.{gen_attempt+1}] CUDA OOM, clearing cache and retrying...\")\n                    torch.cuda.empty_cache()\n                    torch.cuda.ipc_collect()\n                    # Optionally sleep for a short time\n                    import time as _time; _time.sleep(1)\n                    if gen_attempt == 2:\n                        if verbose:\n                            print(f\"[Attempt {i+1}] Bitmap generation failed after retries due to CUDA OOM:\", e)\n                        bitmap = None\n                        continue\n                else:\n                    if verbose:\n                        print(f\"[Attempt {i+1}] Bitmap generation failed:\", e)\n                        import traceback; traceback.print_exc()\n                    bitmap = None\n                    break\n            except Exception as e:\n                if verbose:\n                    print(f\"[Attempt {i+1}] Bitmap generation failed:\", e)\n                    import traceback; traceback.print_exc()\n                bitmap = None\n                break\n        if bitmap is None:\n            continue\n\n        # --- Conversion ---\n        try:\n            t0 = time.time()\n            svg_content = advanced_bitmap_to_svg(bitmap)\n            t1 = time.time()\n            timing['conversion'].append(t1 - t0)\n            if verbose: print(f\"SVG conversion: {t1-t0:.2f}s (size {len(svg_content.encode('utf-8'))} bytes)\")\n        except Exception as e:\n            if verbose:\n                print(f\"[Attempt {i+1}] SVG conversion failed:\", e)\n                import traceback; traceback.print_exc()\n            continue\n\n        # --- Optional Postprocessing ---\n        if postprocess_svg:\n            t0 = time.time()\n            try:\n                svg_content = postprocess_svg(svg_content)\n            except Exception as e:\n                if verbose:\n                    print(f\"[Attempt {i+1}] Postprocessing error:\", e)\n            t1 = time.time()\n            timing['postprocess'].append(t1 - t0)\n            if verbose: print(f\"SVG postprocessing: {t1-t0:.2f}s\")\n\n        # --- Render SVG for evaluation ---\n        try:\n            rendered_svg = svg_to_png(svg_content)\n        except Exception as e:\n            if verbose:\n                print(f\"[Attempt {i+1}] SVG rendering failed:\", e)\n                import traceback; traceback.print_exc()\n            continue\n\n        # --- Display images side by side ---\n        if display_images or verbose:\n            try:\n                import matplotlib.pyplot as plt\n                plt.figure(figsize=(12, 6))\n                plt.subplot(1, 2, 1)\n                plt.imshow(bitmap)\n                plt.title(f\"Original Image {i+1}\")\n                plt.axis('off')\n                plt.subplot(1, 2, 2)\n                plt.imshow(rendered_svg)\n                plt.title(f\"SVG Conversion {i+1}\")\n                plt.axis('off')\n                plt.tight_layout()\n                plt.show()\n            except Exception as e:\n                if verbose:\n                    print(f\"[Attempt {i+1}] Display error:\", e)\n\n        # --- Evaluation (with optional caching) ---\n        cache_key = (svg_content, description)\n        svg_scores = None\n        t0 = time.time()\n        try:\n            if cache is not None and cache_key in cache:\n                svg_scores = cache[cache_key]\n                if verbose: print(\"Loaded score from cache.\")\n            else:\n                svg_scores = evaluate_svg(svg_content, description)\n                if cache is not None:\n                    cache[cache_key] = svg_scores\n            t1 = time.time()\n            timing['evaluation'].append(t1 - t0)\n        except Exception as e:\n            if verbose:\n                print(f\"[Attempt {i+1}] SVG evaluation failed:\", e)\n                import traceback; traceback.print_exc()\n            continue\n\n        # --- Diagnostics ---\n        if svg_scores and verbose:\n            print(f\"Scores: VQA={svg_scores.get('vqa_score',0):.4f}, \"\n                  f\"Aesthetic={svg_scores.get('aesthetic_score',0):.4f}, \"\n                  f\"Combined={svg_scores.get('combined_score',0):.4f}\")\n\n        # --- Save hook ---\n        if save_attempt_hook:\n            try:\n                save_attempt_hook(\n                    attempt_index=i+1,\n                    prompt=prompt,\n                    bitmap=bitmap,\n                    svg_content=svg_content,\n                    svg_scores=svg_scores\n                )\n            except Exception as e:\n                if verbose:\n                    print(f\"[Attempt {i+1}] Save hook error:\", e)\n\n        # --- Track best ---\n        current_score = svg_scores.get('combined_score', 0)\n        attempt_data.append({\n            'svg': svg_content,\n            'score': current_score,\n            'type': 'generated',\n            'scores': svg_scores,\n            'bitmap': bitmap\n        })\n        if current_score > best_score:\n            best_svg = svg_content\n            best_score = current_score\n            if verbose: print(f\"New best result: {current_score:.4f}\")\n        else:\n            if verbose: print(f\"Not better than current best: {best_score:.4f}\")\n\n        timing['attempt'].append(time.time() - attempt_start)\n\n    # --- Summary ---\n    total_time = time.time() - total_start_time\n    if verbose:\n        print(\"\\n=== Timing Summary ===\")\n        for k in ['generation', 'conversion', 'postprocess', 'evaluation', 'attempt']:\n            if timing[k]:\n                print(f\"Avg {k} time: {sum(timing[k])/len(timing[k]):.2f}s\")\n        print(f\"Total pipeline time: {total_time:.2f}s\")\n        print(f\"Best SVG score: {best_score:.4f}\")\n\n    return best_svg, best_score, attempt_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:39:53.969609Z","iopub.execute_input":"2025-05-22T10:39:53.970215Z","iopub.status.idle":"2025-05-22T10:39:54.911869Z","shell.execute_reply.started":"2025-05-22T10:39:53.970156Z","shell.execute_reply":"2025-05-22T10:39:54.91092Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"best_svg, best_score, all_attempts = generate_and_convert_advanced(\n    prompt_prefix=\"Close up of\",\n    description=\"a lighthouse overlooking the ocean\",\n    prompt_suffix=\"flat color blocks, beautiful, minimal details, solid colors only\",\n    negative_prompt=\"lines, hatching, textures, patterns, details, outlines\",\n    num_inference_steps=15,\n    num_attempts=3,\n    verbose=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:39:54.912924Z","iopub.execute_input":"2025-05-22T10:39:54.913228Z","iopub.status.idle":"2025-05-22T10:42:46.734322Z","shell.execute_reply.started":"2025-05-22T10:39:54.913203Z","shell.execute_reply":"2025-05-22T10:42:46.733611Z"}},"outputs":[{"name":"stdout","text":"[Evaluator] Initializing VQA Evaluator...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee6e8fdb38844cf39bd7ada0057d54a3"}},"metadata":{}},{"name":"stdout","text":"[Default SVG 1/3] Scoring error: CUDA out of memory. Tried to allocate 98.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 85.12 MiB is free. Process 2986 has 15.80 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 81.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[Evaluator] Initializing VQA Evaluator...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e75852fbb844449c86681bd83309c6a7"}},"metadata":{}},{"name":"stdout","text":"[Default SVG 2/3] Scoring error: CUDA out of memory. Tried to allocate 1.72 GiB. GPU 0 has a total capacity of 15.89 GiB of which 165.12 MiB is free. Process 2986 has 15.72 GiB memory in use. Of the allocated memory 15.33 GiB is allocated by PyTorch, and 104.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[Evaluator] Initializing VQA Evaluator...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"174de5f0dcb547a2aafd5138f0ac02f8"}},"metadata":{}},{"name":"stdout","text":"[Default SVG 3/3] Scoring error: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 5.12 MiB is free. Process 2986 has 15.88 GiB memory in use. Of the allocated memory 15.56 GiB is allocated by PyTorch, and 35.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\nBest default SVG score: -inf\n\n[Baseline] Best default SVG score: -inf\n\n=== Generation Attempt 1/3 ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41613c404d7448be87b93a69f5daffec"}},"metadata":{}},{"name":"stdout","text":"Bitmap generation: 4.38s\n[Attempt 1] SVG conversion failed: cannot access local variable 'min' where it is not associated with a value\n\n=== Generation Attempt 2/3 ===\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_59/1077370007.py\", line 114, in generate_and_convert_advanced\n    svg_content = advanced_bitmap_to_svg(bitmap)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_59/4291589642.py\", line 157, in advanced_bitmap_to_svg\n    features = extract_advanced_features(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_59/4291589642.py\", line 54, in extract_advanced_features\n    k = min(len(unique_colors), num_colors)\n        ^^^\nUnboundLocalError: cannot access local variable 'min' where it is not associated with a value\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"789a1b8dcd2e4bc7b1a3a472d4f64e6d"}},"metadata":{}},{"name":"stdout","text":"Bitmap generation: 4.36s\n[Attempt 2] SVG conversion failed: cannot access local variable 'min' where it is not associated with a value\n\n=== Generation Attempt 3/3 ===\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_59/1077370007.py\", line 114, in generate_and_convert_advanced\n    svg_content = advanced_bitmap_to_svg(bitmap)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_59/4291589642.py\", line 157, in advanced_bitmap_to_svg\n    features = extract_advanced_features(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_59/4291589642.py\", line 54, in extract_advanced_features\n    k = min(len(unique_colors), num_colors)\n        ^^^\nUnboundLocalError: cannot access local variable 'min' where it is not associated with a value\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a66fac07c5a848edaa6325bbf5a98eb8"}},"metadata":{}},{"name":"stdout","text":"Bitmap generation: 4.36s\n[Attempt 3] SVG conversion failed: cannot access local variable 'min' where it is not associated with a value\n\n=== Timing Summary ===\nAvg generation time: 4.37s\nTotal pipeline time: 170.55s\nBest SVG score: -inf\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/tmp/ipykernel_59/1077370007.py\", line 114, in generate_and_convert_advanced\n    svg_content = advanced_bitmap_to_svg(bitmap)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_59/4291589642.py\", line 157, in advanced_bitmap_to_svg\n    features = extract_advanced_features(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_59/4291589642.py\", line 54, in extract_advanced_features\n    k = min(len(unique_colors), num_colors)\n        ^^^\nUnboundLocalError: cannot access local variable 'min' where it is not associated with a value\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nimport os\n\n\nclass Model:\n    def __init__(self):\n        '''Optional constructor, performs any setup logic, model instantiation, etc.'''\n        self.num_attempts_per_prompt = 2\n        self.num_inference_steps = 15\n        self.guidance_scale = 15\n\n        self.prompt_prefix = \"Close up of\"\n        self.prompt_suffix = \"flat color blocks, beautiful, minimal details, solid colors only\"\n        self.negative_prompt = \"lines, hatching, textures, patterns, details, outlines\"\n\n        self.last_score = None\n\n        self.default_svg = \"\"\"\n        <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"384\" height=\"384\"><rect width=\"100%\" height=\"100%\"/><path d=\"M80 82h224v224H80z\"/><g fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path stroke=\"#FF4700\" stroke-width=\"12\" d=\"M150 199q131-7 45-73\"/><path stroke=\"#000010\" stroke-opacity=\".1\" stroke-width=\"8\" d=\"M270 220q-165-138-99 43\"/><path stroke=\"#FF5E00\" stroke-width=\"4\" d=\"M88 83q-4 70 150 54\"/><path stroke=\"#000\" stroke-opacity=\".6\" stroke-width=\"8\" d=\"M239 232q-80-23 5-136\"/><path stroke=\"#0000C7\" stroke-opacity=\".7\" stroke-width=\"16\" d=\"M202 286q76-84 12-25\"/><path d=\"M223 228q-143-98 56-43\"/><path stroke=\"#00002A\" stroke-opacity=\".6\" stroke-width=\"16\" d=\"M121 268q129 18 124-8\"/><path stroke=\"#8AACFF\" stroke-opacity=\".5\" stroke-width=\"4\" d=\"M304 85q-13 94-138 12\"/><path stroke=\"#04F\" stroke-width=\"12\" d=\"M242 95q-84 96-47 9\"/><path stroke=\"#13FFFF\" stroke-width=\"6\" d=\"M94 166q41-51 72 57\"/><path d=\"M150 134q75 70-5 126\"/><path stroke=\"#FF6300\" stroke-width=\"14\" d=\"M160 103q24-17 80 9\"/><path stroke=\"#446AFF\" stroke-width=\"2\" d=\"M129 232q91 50 156 74\"/><path stroke=\"#00f\" stroke-opacity=\".3\" stroke-width=\"2\" d=\"M120 131q-21-43 67 50\"/><path stroke=\"#fff\" stroke-width=\"12\" d=\"M124 135q26-48-19 29\"/><path stroke=\"#FDA1F6\" stroke-opacity=\".5\" stroke-width=\"10\" d=\"M280 175q23 27-200 100\"/><path stroke=\"#C77145\" stroke-opacity=\".7\" stroke-width=\"16\" d=\"M133 256q132 0 120-87\"/><path stroke=\"#FF0400\" stroke-width=\"12\" d=\"M123 175q-8-33 18 62\"/><path d=\"M135 111q119 59 139 55\"/><path stroke=\"#FF7C00\" stroke-width=\"10\" d=\"M166 250q-30-95 20-168\"/><path stroke=\"#FFE415\" stroke-opacity=\".8\" stroke-width=\"16\" d=\"M169 108q11 77 27 93\"/><path stroke=\"#470C00\" stroke-opacity=\".6\" stroke-width=\"16\" d=\"M234 99q-131 166 2 141\"/><path stroke=\"#040000\" stroke-opacity=\".4\" stroke-width=\"4\" d=\"M92 172q50 11 87 78\"/><path d=\"M143 112q-9 116 93 150\"/><path stroke=\"#fff\" stroke-width=\"12\" d=\"M195 82q-40 0-92 88\"/><path d=\"M169 103q-84 152 42 151\"/><path stroke=\"#fff\" stroke-width=\"14\" d=\"M252 193q18 43-165 38\"/><path stroke=\"#0A61FF\" stroke-opacity=\".6\" stroke-width=\"6\" d=\"M117 176q131-82 132 0\"/><path stroke=\"#09A7E4\" stroke-width=\"14\" d=\"M141 223q123-78 65 29\"/><path stroke=\"#34009E\" stroke-opacity=\".3\" stroke-width=\"16\" d=\"M232 237q43 37 44-72\"/><path d=\"M215 280q88-20 63-88\"/><path stroke=\"#FF4000\" stroke-width=\"10\" d=\"M246 223q-140-71-89 50\"/><path stroke=\"#FFB000\" stroke-width=\"6\" d=\"M187 82q-66 51-103 10\"/><path d=\"M131 168q37 50-27-66\"/><path stroke=\"#FF614F\" stroke-width=\"2\" d=\"M282 248q-158 55 22-22\"/><path stroke=\"#FF9D7A\" stroke-width=\"16\" d=\"M176 153q-44-31-74 109\"/><path stroke=\"#57001C\" stroke-opacity=\".2\" stroke-width=\"16\" d=\"M206 151q66 57-126 131\"/><path stroke=\"#F50048\" stroke-opacity=\".4\" stroke-width=\"16\" d=\"M265 199q-133-75-121-7\"/><path stroke=\"#AE1600\" stroke-opacity=\".7\" stroke-width=\"12\" d=\"M291 256Q84 155 91 165\"/><path stroke=\"#34FFFF\" stroke-width=\"12\" d=\"M95 162q36-67 112-12\"/><path stroke=\"#7B9D6C\" stroke-opacity=\".6\" stroke-width=\"2\" d=\"M271 158q-62 18-177 111\"/><path stroke=\"red\" stroke-width=\"6\" d=\"M211 210q-101-13-128-51\"/><path stroke=\"#005478\" stroke-opacity=\".9\" stroke-width=\"4\" d=\"M118 118Q89 241 232 242\"/><path d=\"M147 230q-58-98-51 25\"/><path stroke=\"#0074FF\" stroke-opacity=\".9\" stroke-width=\"4\" d=\"M154 172q60 68-35 47\"/><path stroke=\"#000574\" stroke-opacity=\".4\" stroke-width=\"16\" d=\"M101 277q182-67 163-142\"/><path stroke=\"#00005E\" stroke-opacity=\".1\" stroke-width=\"14\" d=\"M275 168q-87 27-48-66\"/><path stroke=\"#00B6AB\" stroke-width=\"2\" d=\"M209 84q-115 20 72 83\"/><path stroke=\"#00718B\" stroke-opacity=\".9\" stroke-width=\"14\" d=\"M111 105q93 32-19 70\"/><path d=\"M188 265q-90 16-69 35\"/><path stroke=\"#FDFF00\" stroke-width=\"2\" d=\"M205 214q-82-20-31-132\"/><path d=\"M267 127q33 82 14 135\"/><path stroke=\"#FFFFB7\" stroke-opacity=\".8\" stroke-width=\"8\" d=\"M98 255q153-123 52-149\"/><path stroke=\"#BD003F\" stroke-opacity=\".3\" stroke-width=\"16\" d=\"M270 188Q202 84 80 262\"/><path stroke=\"#EC8500\" stroke-opacity=\".8\" stroke-width=\"16\" d=\"M254 121q-131-35-20 46\"/><path stroke=\"#950200\" stroke-opacity=\".4\" stroke-width=\"10\" d=\"M203 162q87-38 7 103\"/><path stroke=\"#FFD670\" stroke-opacity=\".7\" stroke-width=\"12\" d=\"M249 214q-169-48-131 42\"/><path d=\"M210 226q29-131-75-133\"/><path stroke=\"#FFC559\" stroke-opacity=\".7\" stroke-width=\"6\" d=\"M201 83q76 56 96 38\"/><path d=\"M150 117q70 160 48 131\"/><path stroke=\"#000020\" stroke-width=\"4\" d=\"M264 221q37-65-13-38\"/><path stroke=\"#595987\" stroke-opacity=\".6\" stroke-width=\"12\" d=\"M248 246q-38-90 13-81\"/><path stroke=\"#0344A0\" stroke-opacity=\".6\" stroke-width=\"16\" d=\"M80 265q204-51 1-26\"/><path stroke=\"#F60\" stroke-width=\"10\" d=\"M298 239q-168-31-160-12\"/><path stroke=\"#A47686\" stroke-opacity=\".5\" stroke-width=\"14\" d=\"M273 152q-92-70-136 32\"/><path stroke=\"#FFAF67\" stroke-width=\"16\" d=\"M166 129q1-46-47 79\"/><path stroke=\"#00000A\" stroke-opacity=\".5\" stroke-width=\"6\" d=\"M304 162q-35-36-89-14\"/><path stroke=\"#1E167F\" stroke-width=\"10\" d=\"M302 242q-86-26-49-56\"/><path stroke=\"#620114\" stroke-opacity=\".9\" stroke-width=\"2\" d=\"M248 257q-14-145-56-160\"/><path stroke=\"#C6000D\" stroke-opacity=\".3\" stroke-width=\"2\" d=\"M217 82q-39 116-128 85\"/><path stroke=\"#000\" stroke-opacity=\".7\" stroke-width=\"4\" d=\"M109 306q-28-3-20-224\"/><path stroke=\"#ff0\" stroke-opacity=\".2\" stroke-width=\"6\" d=\"M165 96q6 141 57 117\"/><path stroke=\"#00636C\" stroke-width=\"14\" d=\"M118 165q67 89 16 77\"/><path stroke=\"#F03A00\" stroke-opacity=\".5\" stroke-width=\"6\" d=\"M253 162q-136-61 51-27\"/><path stroke=\"#275E99\" stroke-opacity=\".6\" stroke-width=\"16\" d=\"M227 306q-6-72-111-145\"/><path d=\"M250 167q-22-15-169 61\"/><path stroke=\"#01F0C3\" stroke-opacity=\".6\" stroke-width=\"14\" d=\"M217 199q8 1-118 53\"/><path stroke=\"#FDFF80\" stroke-opacity=\".3\" stroke-width=\"14\" d=\"M147 134q108-52 65-33\"/><path stroke=\"#5E448B\" stroke-opacity=\".4\" stroke-width=\"16\" d=\"M198 306q-57-143-60-129\"/><path d=\"M112 135q148 110 13 96\"/><path stroke=\"#798CBA\" stroke-opacity=\".6\" stroke-width=\"4\" d=\"M245 231q-115 2 34-72\"/><path stroke=\"#FFFFD4\" stroke-width=\"4\" d=\"M92 158q195 102 157 90\"/><path stroke=\"#0093FF\" stroke-opacity=\".8\" stroke-width=\"12\" d=\"M131 228q-38-33 77-4\"/><path stroke=\"#F25A27\" stroke-opacity=\".3\" stroke-width=\"14\" d=\"M204 211q72-115-83 0\"/><path stroke=\"#1A4BE6\" stroke-opacity=\".5\" stroke-width=\"14\" d=\"M227 306q-122-190 77-12\"/><path stroke=\"#04737C\" stroke-opacity=\".5\" stroke-width=\"16\" d=\"M227 123q-56 42-123 41\"/><path stroke=\"#FF9F10\" stroke-opacity=\".9\" stroke-width=\"4\" d=\"M135 106q-48 69 144 133\"/><path stroke=\"#8C8086\" stroke-opacity=\".4\" stroke-width=\"8\" d=\"M296 116q-182 47-102 5\"/><path stroke=\"#00F6FF\" stroke-width=\"2\" d=\"M243 159q-63 24 61-22\"/><path stroke=\"#F97658\" stroke-opacity=\".4\" stroke-width=\"12\" d=\"M209 120q-91 68-110 134\"/><path stroke=\"#FF3600\" stroke-width=\"12\" d=\"M174 206q20-109 73-30\"/><path stroke=\"#8A0000\" stroke-width=\"4\" d=\"M106 92q46 98 116 122\"/><path stroke=\"#817E60\" stroke-opacity=\".9\" stroke-width=\"12\" d=\"M188 158q34 82-67-20\"/><path stroke=\"#000040\" stroke-width=\"16\" d=\"M115 274q189-68 35-118\"/><path stroke=\"#976000\" stroke-opacity=\".4\" stroke-width=\"12\" d=\"M277 169q-38-26-182 73\"/><path stroke=\"#59693F\" stroke-opacity=\".6\" stroke-width=\"10\" d=\"M158 158q129 98-25 30\"/></g><rect width=\"100%\" height=\"80\"/><rect width=\"100%\" height=\"80\" y=\"304\"/><rect width=\"80\" height=\"100%\"/><rect width=\"80\" height=\"100%\" x=\"304\"/><g stroke=\"#616161\"><path d=\"m140 320.5 4 7m0 0v5.5m0-5.5 4-7\"/><path id=\"a\" d=\"M172.6 324v3.5m0 0v9.5m0-9.5c8-13 7.4 14.5 0 1.5\"/><use x=\"21\" y=\"36\" href=\"#a\"/><path id=\"b\" d=\"M164.774 324.408s-9.472-.045-3 4c8 5-3 4-3 4\"/><use x=\"60\" y=\"36\" href=\"#b\"/><use x=\"71\" href=\"#b\"/><use x=\"77\" y=\"18\" href=\"#b\"/><path id=\"c\" d=\"M149.5 364.5c0 2.5-1.5 4-3 4s-3-1.5-3-4 1.5-4 3-4 3 1.5 3 4Z\"/><use x=\"20\" y=\"-18\" href=\"#c\"/><use x=\"39\" y=\"-36\" href=\"#c\"/><path id=\"d\" d=\"M150 328c1-8 7-2 6 0zm0 0c0 6 6 4 6 4\"/><use x=\"78\" y=\"36\" href=\"#d\"/><use x=\"60\" y=\"36\" href=\"#d\"/><use x=\"14\" y=\"36\" href=\"#d\"/><use x=\"49\" y=\"18\" href=\"#d\"/><use x=\"72\" y=\"18\" href=\"#d\"/><path id=\"f\" d=\"M192 323v3m0 7v-7m0 0 4-3\"/><use x=\"11\" y=\"36\" href=\"#f\"/><use x=\"39\" y=\"18\" href=\"#f\"/><use x=\"14\" href=\"#f\"/><path id=\"g\" d=\"M146 343h2m4 0h-4m0 0v-3m0 3v7.5l3.5.5\"/><use x=\"52\" y=\"-18\" href=\"#g\"/><use x=\"38\" href=\"#g\"/><use x=\"10\" y=\"18\" href=\"#g\"/><use x=\"101\" y=\"18\" href=\"#g\"/><use x=\"36\" y=\"18\" href=\"#g\"/><use x=\"61\" href=\"#g\"/><use x=\"68\" href=\"#g\"/><path d=\"M218 333v-3m-5-6c6-2 5 4 5 4m0 0s-6-2-6 2c1 5 6 0 6 0m0-2v2M221 324l4 9m0 0 3-9m-3 9-4 4M128.5 341.5l2.5 8.5 3-7.5 3.5 7.5 2.5-8.5M196 338v14M153.5 351v-7m0-7v7m0 0s7-7.5 6.5 7\"/><path id=\"h\" d=\"M180.5 342v6.5m0 2.5v-2.5m0 0s-7.5 7.5-6.5-7\"/><use x=\"64\" y=\"18\" href=\"#h\" transform=\"rotate(180 241 364)\"/><path d=\"M143 342v9m0-11v-2M131 369v-11l8 11v-12M173 369l3-5m4-4-4 4m0 0-3-4m3 4 3 5M239.5 331l-1 4M246.5 349l-1 4\"/></g></svg>\"\"\"\n\n    def predict(self, description: str) -> str:\n        '''Generates SVG which produces an image described by the prompt.\n\n        Args:\n            description (str): A prompt describing an image\n        Returns:\n            String of valid SVG code.\n        '''\n        best_svg = self.default_svg\n        try:\n            # Use the advanced, OOM-aware function if available\n            best_svg, best_score, _ = generate_and_convert_advanced(\n                prompt_prefix=self.prompt_prefix, \n                description=description, \n                prompt_suffix=self.prompt_suffix,\n                negative_prompt=self.negative_prompt,\n                num_attempts=self.num_attempts_per_prompt,\n                num_inference_steps=self.num_inference_steps,\n                guidance_scale=self.guidance_scale,\n                verbose=False,\n                fallback_svg=self.default_svg\n            )\n            self.last_score = best_score\n        except Exception as e:\n            # Optionally print(e) for debugging\n            pass\n        return best_svg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:00:17.105117Z","iopub.status.idle":"2025-05-22T10:00:17.105339Z","shell.execute_reply.started":"2025-05-22T10:00:17.105231Z","shell.execute_reply":"2025-05-22T10:00:17.105242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom datetime import timedelta\n\n# Read the CSV file\ndf = pd.read_csv('/kaggle/input/drawing-with-llms/train.csv')\n\n# Uncomment to test on just a few\n# df = df.head(3)\n\n# Initialize the model\nmodel = Model()\n\n# Create arrays to store scores and timing data\nscores = []\ngeneration_times = []\n\nfor i, row in enumerate(df.iterrows()):\n    description = row[1]['description']\n    \n    # Pre-generation: clear CUDA cache to reduce fragmentation\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n    \n    # Start timing\n    start_time = time.time()\n    \n    # Generate image from description, handle OOM and general errors per sample\n    try:\n        svg = model.predict(description)\n        rendered_img = svg_to_png(svg)\n        score = model.last_score\n    except RuntimeError as e:\n        if \"CUDA out of memory\" in str(e):\n            print(f\"[{i+1}] CUDA OOM - skipping and clearing cache.\")\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n            scores.append(float('nan'))\n            generation_times.append(float('nan'))\n            continue\n        else:\n            print(f\"[{i+1}] Generation failed: {e}\")\n            scores.append(float('nan'))\n            generation_times.append(float('nan'))\n            continue\n    except Exception as e:\n        print(f\"[{i+1}] Unexpected error: {e}\")\n        scores.append(float('nan'))\n        generation_times.append(float('nan'))\n        continue\n\n    # End timing\n    end_time = time.time()\n    generation_time = end_time - start_time\n    generation_times.append(generation_time)\n    scores.append(score)\n        \n    # Display the image being processed\n    plt.figure(figsize=(10, 8))\n    plt.imshow(rendered_img)\n    plt.title(f\"Prompt: {description}\\nScore: {score:.2f}\")\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n    \n    # Print progress, current average score, and timing info\n    current_avg_score = np.nanmean(scores)\n    current_avg_time = np.nanmean(generation_times)\n    \n    print(f\"Processed {i+1}/{len(df)} images\")\n    print(f\"Current average score: {current_avg_score:.2f}\")\n    print(f\"Time for this image: {generation_time:.2f}s\")\n    print(f\"Current average generation time: {current_avg_time:.2f}s\")\n    \n    # Post-generation: clear cache again\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n    \n# When all done, calculate final statistics\navg_score = np.nanmean(scores)\navg_generation_time = np.nanmean(generation_times)\ntotal_time_taken = np.nansum(generation_times)\n\n# Calculate projections for 500 images\nprojected_time_500_images = 500 * avg_generation_time\nprojected_hours = projected_time_500_images / 3600\n\nprint(\"\\n=== SUMMARY ===\")\nprint(f\"Images processed: {len(df)}\")\nprint(f\"Final average score: {avg_score:.2f}\")\nprint(f\"Average generation time per image: {avg_generation_time:.2f} seconds\")\nprint(f\"Total time elapsed: {timedelta(seconds=total_time_taken)}\")\nprint(f\"Projected time for 500 images: {projected_hours:.2f} hours ({timedelta(seconds=projected_time_500_images)})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-22T10:00:17.106068Z","iopub.status.idle":"2025-05-22T10:00:17.106313Z","shell.execute_reply.started":"2025-05-22T10:00:17.10621Z","shell.execute_reply":"2025-05-22T10:00:17.106221Z"}},"outputs":[],"execution_count":null}]}