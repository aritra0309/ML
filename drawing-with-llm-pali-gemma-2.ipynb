{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":89659,"databundleVersionId":11735795,"sourceType":"competition"},{"sourceId":224423433,"sourceType":"kernelVersion"},{"sourceId":263093,"sourceType":"modelInstanceVersion","modelInstanceId":225001,"modelId":164716}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aritra423/drawing-with-llm-pali-gemma-2?scriptVersionId=240214074\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppresses INFO and WARNING logs\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:09:49.576989Z","iopub.execute_input":"2025-05-17T06:09:49.577446Z","iopub.status.idle":"2025-05-17T06:09:49.58208Z","shell.execute_reply.started":"2025-05-17T06:09:49.57742Z","shell.execute_reply":"2025-05-17T06:09:49.581147Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"Kaagle Metric\n","metadata":{}},{"cell_type":"code","source":"import ast\nimport io\nimport math\nimport statistics\nimport string\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image, ImageFilter\nimport cairosvg\nimport clip\nimport torch\nimport torch.nn as nn\nimport cv2\nfrom more_itertools import chunked\n\nimport kagglehub\nsvg_constraints = kagglehub.package_import('metric/svg-constraints')\n\nclass ParticipantVisibleError(Exception):\n    pass\n\ndef score(\n    solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, random_seed: int = 0\n) -> float:\n    for colname in ['question', 'choices', 'answer']:\n        solution[colname] = solution[colname].apply(ast.literal_eval)\n    solution = solution.explode(['question', 'choices', 'answer'])\n\n    if not pd.api.types.is_string_dtype(submission.loc[:, 'svg']):\n        raise ParticipantVisibleError('svg must be a string.')\n\n    constraints = svg_constraints.SVGConstraints()\n    try:\n        for svg in submission.loc[:, 'svg']:\n            constraints.validate_svg(svg)\n    except Exception:\n        raise ParticipantVisibleError('SVG code violates constraints.')\n\n    vqa_evaluator = VQAEvaluator()\n    aesthetic_evaluator = AestheticEvaluator()\n\n    results = []\n    rng = np.random.RandomState(random_seed)\n    try:\n        df = solution.merge(submission, on='id')\n        for i, (_, group) in enumerate(df.loc[\n            :, ['id', 'question', 'choices', 'answer', 'svg']\n        ].groupby('id')):\n            questions, choices, answers, svg = [\n                group[col_name].to_list()\n                for col_name in group.drop('id', axis=1).columns\n            ]\n            svg = svg[0]\n            group_seed = rng.randint(0, np.iinfo(np.int32).max)\n            image_processor = ImageProcessor(image=svg_to_png(svg), seed=group_seed).apply()\n            image = image_processor.image.copy()\n            aesthetic_score = aesthetic_evaluator.score(image)\n            vqa_score = vqa_evaluator.score(questions, choices, answers, image)\n            image_processor.reset().apply_random_crop_resize().apply_jpeg_compression(quality=90)\n            ocr_score = vqa_evaluator.ocr(image_processor.image)\n            instance_score = (\n                harmonic_mean(vqa_score, aesthetic_score, beta=0.5) * ocr_score\n            )\n            results.append(instance_score)\n    except Exception:\n        raise ParticipantVisibleError('SVG failed to score.')\n\n    fidelity = statistics.mean(results)\n    return float(fidelity)\n\nclass VQAEvaluator:\n    def __init__(self):\n        from transformers import (\n            AutoProcessor,\n            BitsAndBytesConfig,\n            PaliGemmaForConditionalGeneration,\n        )\n        self.quantization_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type='nf4',\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.float16,\n        )\n        self.letters = string.ascii_uppercase\n        self.model_path = kagglehub.model_download(\n            'google/paligemma-2/transformers/paligemma2-10b-mix-448'\n        )\n        self.processor = AutoProcessor.from_pretrained(self.model_path)\n        self.model = PaliGemmaForConditionalGeneration.from_pretrained(\n            self.model_path,\n            low_cpu_mem_usage=True,\n            quantization_config=self.quantization_config,\n        ).to('cuda:0')\n\n    def score(self, questions, choices, answers, image, n=4):\n        scores = []\n        batches = (chunked(qs, n) for qs in [questions, choices, answers])\n        for question_batch, choice_batch, answer_batch in zip(*batches, strict=True):\n            scores.extend(\n                self.score_batch(\n                    image,\n                    question_batch,\n                    choice_batch,\n                    answer_batch,\n                )\n            )\n        return statistics.mean(scores)\n\n    def score_batch(\n        self,\n        image: Image.Image,\n        questions: list[str],\n        choices_list: list[list[str]],\n        answers: list[str],\n    ) -> list[float]:\n        prompts = [\n            self.format_prompt(question, choices)\n            for question, choices in zip(questions, choices_list, strict=True)\n        ]\n        batched_choice_probabilities = self.get_choice_probability(\n            image, prompts, choices_list\n        )\n\n        scores = []\n        for i, _ in enumerate(questions):\n            choice_probabilities = batched_choice_probabilities[i]\n            answer = answers[i]\n            answer_probability = 0.0\n            for choice, prob in choice_probabilities.items():\n                if choice == answer:\n                    answer_probability = prob\n                    break\n            scores.append(answer_probability)\n\n        return scores\n\n    def format_prompt(self, question: str, choices: list[str]) -> str:\n        prompt = f'<image>answer en Question: {question}\\nChoices:\\n'\n        for i, choice in enumerate(choices):\n            prompt += f'{self.letters[i]}. {choice}\\n'\n        return prompt\n\n    def mask_choices(self, logits, choices_list):\n        batch_size = logits.shape[0]\n        masked_logits = torch.full_like(logits, float('-inf'))\n\n        for batch_idx in range(batch_size):\n            choices = choices_list[batch_idx]\n            for i in range(len(choices)):\n                letter_token = self.letters[i]\n                first_token = self.processor.tokenizer.encode(\n                    letter_token, add_special_tokens=False\n                )[0]\n                first_token_with_space = self.processor.tokenizer.encode(\n                    ' ' + letter_token, add_special_tokens=False\n                )[0]\n\n                if isinstance(first_token, int):\n                    masked_logits[batch_idx, first_token] = logits[\n                        batch_idx, first_token\n                    ]\n                if isinstance(first_token_with_space, int):\n                    masked_logits[batch_idx, first_token_with_space] = logits[\n                        batch_idx, first_token_with_space\n                    ]\n\n        return masked_logits\n\n    def get_choice_probability(self, image, prompts, choices_list) -> list[dict]:\n        inputs = self.processor(\n            images=[image] * len(prompts),\n            text=prompts,\n            return_tensors='pt',\n            padding='longest',\n        ).to('cuda:0')\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            logits = outputs.logits[:, -1, :]\n            masked_logits = self.mask_choices(logits, choices_list)\n            probabilities = torch.softmax(masked_logits, dim=-1)\n\n        batched_choice_probabilities = []\n        for batch_idx in range(len(prompts)):\n            choice_probabilities = {}\n            choices = choices_list[batch_idx]\n            for i, choice in enumerate(choices):\n                letter_token = self.letters[i]\n                first_token = self.processor.tokenizer.encode(\n                    letter_token, add_special_tokens=False\n                )[0]\n                first_token_with_space = self.processor.tokenizer.encode(\n                    ' ' + letter_token, add_special_tokens=False\n                )[0]\n\n                prob = 0.0\n                if isinstance(first_token, int):\n                    prob += probabilities[batch_idx, first_token].item()\n                if isinstance(first_token_with_space, int):\n                    prob += probabilities[batch_idx, first_token_with_space].item()\n                choice_probabilities[choice] = prob\n\n            total_prob = sum(choice_probabilities.values())\n            if total_prob > 0:\n                renormalized_probabilities = {\n                    choice: prob / total_prob\n                    for choice, prob in choice_probabilities.items()\n                }\n            else:\n                renormalized_probabilities = choice_probabilities\n            batched_choice_probabilities.append(renormalized_probabilities)\n\n        return batched_choice_probabilities\n\n    def ocr(self, image, free_chars=4):\n        inputs = (\n            self.processor(\n                text='<image>ocr\\n',\n                images=image,\n                return_tensors='pt',\n            )\n            .to(torch.float16)\n            .to(self.model.device)\n        )\n        input_len = inputs['input_ids'].shape[-1]\n\n        with torch.inference_mode():\n            outputs = self.model.generate(**inputs, max_new_tokens=32, do_sample=False)\n            outputs = outputs[0][input_len:]\n            decoded = self.processor.decode(outputs, skip_special_tokens=True)\n\n        num_char = len(decoded)\n        return min(1.0, math.exp(-num_char + free_chars))\n\nclass AestheticPredictor(nn.Module):\n    def __init__(self, input_size):\n        super().__init__()\n        self.input_size = input_size\n        self.layers = nn.Sequential(\n            nn.Linear(self.input_size, 1024),\n            nn.Dropout(0.2),\n            nn.Linear(1024, 128),\n            nn.Dropout(0.2),\n            nn.Linear(128, 64),\n            nn.Dropout(0.1),\n            nn.Linear(64, 16),\n            nn.Linear(16, 1),\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\nclass AestheticEvaluator:\n    def __init__(self):\n        self.model_path = '/kaggle/input/sac-logos-ava1-l14-linearmse/sac+logos+ava1-l14-linearMSE.pth'\n        self.clip_model_path = '/kaggle/input/openai-clip-vit-large-patch14/ViT-L-14.pt'\n        self.predictor, self.clip_model, self.preprocessor = self.load()\n\n    def load(self):\n        state_dict = torch.load(self.model_path, weights_only=True, map_location='cuda:1')\n        predictor = AestheticPredictor(768)\n        predictor.load_state_dict(state_dict)\n        predictor.to('cuda:1')\n        predictor.eval()\n        clip_model, preprocessor = clip.load(self.clip_model_path, device='cuda:1')\n        return predictor, clip_model, preprocessor\n\n    def score(self, image: Image.Image) -> float:\n        image = self.preprocessor(image).unsqueeze(0).to('cuda:1')\n        with torch.no_grad():\n            image_features = self.clip_model.encode_image(image)\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n            image_features = image_features.cpu().detach().numpy()\n        score = self.predictor(torch.from_numpy(image_features).to('cuda:1').float())\n        return score.item() / 10.0\n\ndef harmonic_mean(a: float, b: float, beta: float = 1.0) -> float:\n    if a <= 0 or b <= 0:\n        return 0.0\n    return (1 + beta**2) * (a * b) / (beta**2 * a + b)\n\ndef svg_to_png(svg_code: str, size: tuple = (384, 384)) -> Image.Image:\n    if 'viewBox' not in svg_code:\n        svg_code = svg_code.replace('<svg', f'<svg viewBox=\"0 0 {size[0]} {size[1]}\"')\n    png_data = cairosvg.svg2png(bytestring=svg_code.encode('utf-8'))\n    return Image.open(io.BytesIO(png_data)).convert('RGB').resize(size)\n\nclass ImageProcessor:\n    def __init__(self, image: Image.Image, seed=None):\n        self.image = image\n        self.original_image = self.image.copy()\n        if seed is not None:\n            self.rng = np.random.RandomState(seed)\n        else:\n            self.rng = np.random\n\n    def reset(self):\n        self.image = self.original_image.copy()\n        return self\n\n    def apply_median_filter(self, size=3):\n        self.image = self.image.filter(ImageFilter.MedianFilter(size=size))\n        return self\n\n    def apply_bilateral_filter(self, d=9, sigma_color=75, sigma_space=75):\n        img_array = np.asarray(self.image)\n        filtered = cv2.bilateralFilter(img_array, d, sigma_color, sigma_space)\n        self.image = Image.fromarray(filtered)\n        return self\n\n    def apply_fft_low_pass(self, cutoff_frequency=0.5):\n        img_array = np.array(self.image, dtype=np.float32)\n        result = np.zeros_like(img_array)\n        for i in range(3):\n            f = np.fft.fft2(img_array[:, :, i])\n            fshift = np.fft.fftshift(f)\n            rows, cols = img_array[:, :, i].shape\n            crow, ccol = rows // 2, cols // 2\n            mask = np.zeros((rows, cols), np.float32)\n            r = int(min(crow, ccol) * cutoff_frequency)\n            center = [crow, ccol]\n            x, y = np.ogrid[:rows, :cols]\n            mask_area = (x - center[0]) ** 2 + (y - center[1]) ** 2 <= r * r\n            mask[mask_area] = 1\n            fshift_filtered = fshift * mask\n            f_ishift = np.fft.ifftshift(fshift_filtered)\n            img_back = np.fft.ifft2(f_ishift)\n            img_back = np.real(img_back)\n            result[:, :, i] = img_back\n        result = np.clip(result, 0, 255).astype(np.uint8)\n        self.image = Image.fromarray(result)\n        return self\n\n    def apply_jpeg_compression(self, quality=85):\n        buffer = io.BytesIO()\n        self.image.save(buffer, format='JPEG', quality=quality)\n        buffer.seek(0)\n        self.image = Image.open(buffer)\n        return self\n\n    def apply_random_crop_resize(self, crop_percent=0.05):\n        width, height = self.image.size\n        crop_pixels_w = int(width * crop_percent)\n        crop_pixels_h = int(height * crop_percent)\n        left = self.rng.randint(0, crop_pixels_w + 1)\n        top = self.rng.randint(0, crop_pixels_h + 1)\n        right = width - self.rng.randint(0, crop_pixels_w + 1)\n        bottom = height - self.rng.randint(0, crop_pixels_h + 1)\n        self.image = self.image.crop((left, top, right, bottom))\n        self.image = self.image.resize((width, height), Image.BILINEAR)\n        return self\n\n    def apply(self):\n        return (\n            self.apply_random_crop_resize(crop_percent=0.03)\n            .apply_jpeg_compression(quality=95)\n            .apply_median_filter(size=9)\n            .apply_fft_low_pass(cutoff_frequency=0.5)\n            .apply_bilateral_filter(d=5, sigma_color=75, sigma_space=75)\n            .apply_jpeg_compression(quality=92)\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoProcessor, BitsAndBytesConfig, PaliGemmaForConditionalGeneration\nimport kagglehub\n\nprint(\"Using device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel_path = kagglehub.model_download('google/paligemma-2/transformers/paligemma2-10b-mix-448')\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nprocessor = AutoProcessor.from_pretrained(model_path,use_fast=False)\nmodel = PaliGemmaForConditionalGeneration.from_pretrained(\n    model_path,\n    quantization_config=quantization_config,\n    torch_dtype=torch.float16,\n    low_cpu_mem_usage=True,\n    device_map=\"auto\",\n)\n\nprint(\"PaLiGemma 2 loaded.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from diffusers import StableDiffusionPipeline\n\npipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\npipe = pipe.to(\"cuda\")  # Or \"cpu\" if you don't have a GPU","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom PIL import Image\nfrom skimage.segmentation import slic\nfrom skimage.measure import regionprops, label\nfrom skimage.color import rgb2lab, rgb2gray\nfrom skimage.filters import sobel\n\ndef compress_hex_color(hex_color):\n    r, g, b = int(hex_color[1:3], 16), int(hex_color[3:5], 16), int(hex_color[5:7], 16)\n    if r % 17 == 0 and g % 17 == 0 and b % 17 == 0:\n        return f'#{r//17:x}{g//17:x}{b//17:x}'\n    return hex_color\n\ndef fit_polygon(contour, max_error=2.0):\n    \"\"\"Fit a polygon to the given contour using the Ramer-Douglas-Peucker algorithm.\"\"\"\n    epsilon = max_error\n    approx = cv2.approxPolyDP(contour, epsilon, True)\n    return approx\n\ndef fit_ellipse(contour):\n    \"\"\"Fit an ellipse to the given contour if possible.\"\"\"\n    if len(contour) < 5:\n        return None\n    ellipse = cv2.fitEllipse(contour)\n    return ellipse\n\ndef color_contrast(color1, color2):\n    # Use Euclidean distance in LAB color space for perceptual contrast\n    l1 = rgb2lab(np.array([[color1]], dtype=np.uint8)/255.0)[0,0]\n    l2 = rgb2lab(np.array([[color2]], dtype=np.uint8)/255.0)[0,0]\n    return np.linalg.norm(l1 - l2)\n\ndef extract_advanced_features(img_np, max_superpixels=200, num_colors=16):\n    \"\"\"Multi-scale segmentation, edge detection, and saliency-weighted feature extraction.\"\"\"\n    # Ensure RGB\n    if img_np.shape[2] == 4:\n        img_np = img_np[:,:,:3]\n    img_rgb = img_np if img_np.shape[2] == 3 else cv2.cvtColor(img_np, cv2.COLOR_GRAY2RGB)\n    \n    height, width = img_rgb.shape[:2]\n    img_lab = rgb2lab(img_rgb/255.0)\n    img_gray = rgb2gray(img_rgb)\n    edge_map = sobel(img_gray)\n    # SLIC superpixel segmentation for multi-scale features\n    segments = slic(img_rgb, n_segments=max_superpixels, compactness=10, start_label=1)\n    region_labels = label(segments)\n    regions = regionprops(region_labels)\n    \n    # K-means color quantization\n    pixels = img_rgb.reshape(-1, 3).astype(np.float32)\n    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n    _, labels, centers = cv2.kmeans(pixels, num_colors, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n    palette = centers.astype(np.uint8)\n    quantized = palette[labels.flatten()].reshape(img_rgb.shape)\n    \n    # Saliency: edge strength, area, color contrast, and spatial centrality\n    center_x, center_y = width/2, height/2\n    features = []\n    \n    for region in regions:\n        minr, minc, maxr, maxc = region.bbox\n        mask = (region_labels == region.label).astype(np.uint8)\n        contour_list, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        if len(contour_list) == 0:\n            continue\n        contour = max(contour_list, key=cv2.contourArea)\n        area = cv2.contourArea(contour)\n        if area < 30:\n            continue\n        # Dominant color in this region (mode of quantized image in region)\n        region_pixels = quantized[mask.astype(bool)]\n        if len(region_pixels) == 0:\n            continue\n        unique, counts = np.unique(region_pixels.reshape(-1,3), axis=0, return_counts=True)\n        dom_color = unique[counts.argmax()]\n        hex_color = compress_hex_color(f'#{dom_color[0]:02x}{dom_color[1]:02x}{dom_color[2]:02x}')\n        \n        # Fit a polygon (could also try ellipse or Bezier)\n        poly = fit_polygon(contour, max_error=3.0)\n        if len(poly) < 3:\n            continue\n        points = \" \".join([f\"{pt[0][0]:.2f},{pt[0][1]:.2f}\" for pt in poly])\n        \n        # Optionally fit ellipse if region is roughly elliptical\n        ellipse_svg = \"\"\n        ellipse = fit_ellipse(contour)\n        if ellipse is not None and area / (np.pi * ellipse[1][0]/2 * ellipse[1][1]/2) > 0.85:\n            (cx, cy), (maj, min), angle = ellipse\n            ellipse_svg = f'<ellipse cx=\"{cx:.2f}\" cy=\"{cy:.2f}\" rx=\"{maj/2:.2f}\" ry=\"{min/2:.2f}\" fill=\"{hex_color}\" transform=\"rotate({angle:.1f} {cx:.2f} {cy:.2f})\"/>'\n        \n        # Importance: area, edge saliency, color contrast to mean bg, centrality\n        m = cv2.moments(contour)\n        cx = int(m[\"m10\"] / m[\"m00\"]) if m[\"m00\"] > 0 else (minc+maxc)/2\n        cy = int(m[\"m01\"] / m[\"m00\"]) if m[\"m00\"] > 0 else (minr+maxr)/2\n        dist_from_center = np.sqrt(((cx - center_x) / width)**2 + ((cy - center_y) / height)**2)\n        mean_edge = np.mean(edge_map[mask.astype(bool)])\n        bg_color = np.mean(img_rgb, axis=(0,1)).astype(int)\n        contrast = color_contrast(dom_color, bg_color)\n        importance = (area * (1 - dist_from_center) * (mean_edge + 0.2) * (contrast+5))\n        features.append({\n            'points': points,\n            'color': hex_color,\n            'area': area,\n            'importance': importance,\n            'ellipse_svg': ellipse_svg\n        })\n    features.sort(key=lambda x: x['importance'], reverse=True)\n    return features\n\ndef advanced_bitmap_to_svg(\n    image,\n    max_size_bytes=20000,\n    resize=True,\n    target_size=(384, 384),\n    max_superpixels=200,\n    num_colors=16,\n    use_ellipses=True,\n    group_similar=True,\n):\n    \"\"\"\n    Convert a bitmap PIL image to an advanced, high-fidelity SVG string.\n    \"\"\"\n    if resize:\n        original_size = image.size\n        image = image.resize(target_size, Image.LANCZOS)\n    else:\n        original_size = image.size\n    img_np = np.array(image)\n    height, width = img_np.shape[:2]\n    avg_bg_color = np.mean(img_np[:,:,:3], axis=(0,1)).astype(int) if img_np.shape[2] >= 3 else 255\n    bg_hex_color = compress_hex_color(f'#{avg_bg_color[0]:02x}{avg_bg_color[1]:02x}{avg_bg_color[2]:02x}')\n    orig_width, orig_height = original_size\n    svg = [f'<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"{orig_width}\" height=\"{orig_height}\" viewBox=\"0 0 {width} {height}\">']\n    svg.append(f'<rect width=\"{width}\" height=\"{height}\" fill=\"{bg_hex_color}\"/>')\n    features = extract_advanced_features(\n        img_np,\n        max_superpixels=max_superpixels,\n        num_colors=num_colors\n    )\n    # Optionally group polygons by color for smaller SVG\n    if group_similar:\n        from collections import defaultdict\n        color_groups = defaultdict(list)\n        for f in features:\n            color_groups[f['color']].append(f)\n        for color, feats in sorted(color_groups.items(), key=lambda x: -sum(f['importance'] for f in x[1])):\n            svg.append(f'<g fill=\"{color}\">')\n            for feat in feats:\n                if use_ellipses and feat['ellipse_svg']:\n                    svg.append(feat['ellipse_svg'])\n                svg.append(f'<polygon points=\"{feat[\"points\"]}\"/>')\n            svg.append('</g>')\n    else:\n        for feat in features:\n            if use_ellipses and feat['ellipse_svg']:\n                svg.append(feat['ellipse_svg'])\n            svg.append(f'<polygon points=\"{feat[\"points\"]}\" fill=\"{feat[\"color\"]}\"/>')\n    svg.append('</svg>')\n    svg_str = \"\".join(svg)\n    # Truncate if needed\n    if len(svg_str.encode('utf-8')) > max_size_bytes:\n        svg_str = f'<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"{orig_width}\" height=\"{orig_height}\" viewBox=\"0 0 {width} {height}\"><rect width=\"{width}\" height=\"{height}\" fill=\"{bg_hex_color}\"/></svg>'\n    return svg_str","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\ndef generate_bitmap_advanced(\n    prompt,\n    negative_prompt=\"\",\n    num_inference_steps=20,\n    guidance_scale=15,\n    pipe=None,\n    device=None,\n    verbose=False,\n    seed=None,\n    **extra_kwargs\n):\n    \"\"\"\n    Generate an image using a diffusion pipeline with advanced options.\n    \n    Parameters:\n        prompt (str): The prompt for image generation.\n        negative_prompt (str): Features to avoid.\n        num_inference_steps (int): Number of diffusion steps.\n        guidance_scale (float): Classifier-free guidance scale.\n        pipe: Diffusion pipeline object (if None, uses global 'pipe').\n        device: Device for inference (if not None, moves pipe to device).\n        verbose (bool): If True, prints diagnostic information.\n        seed (int or None): Optional random seed for reproducibility.\n        extra_kwargs: Any additional keyword arguments for the pipeline.\n        \n    Returns:\n        image: The generated image (PIL.Image or similar).\n    \"\"\"\n    import torch\n\n    if pipe is None:\n        try:\n            pipe = globals()[\"pipe\"]\n        except KeyError:\n            raise ValueError(\"No 'pipe' diffusion pipeline provided or defined globally.\")\n\n    if device is not None:\n        pipe = pipe.to(device)\n        if verbose:\n            print(f\"Pipeline moved to device: {device}\")\n\n    generator = None\n    if seed is not None:\n        generator = torch.Generator(device=device if device else \"cpu\").manual_seed(seed)\n        if verbose:\n            print(f\"Using random seed: {seed}\")\n\n    if verbose:\n        print(f\"Prompt: {prompt}\")\n        print(f\"Negative prompt: {negative_prompt}\")\n        print(f\"Inference steps: {num_inference_steps}\")\n        print(f\"Guidance scale: {guidance_scale}\")\n\n    result = pipe(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n        generator=generator,\n        **extra_kwargs\n    )\n    image = result.images[0]\n    if verbose:\n        print(\"Image generated successfully.\")\n    return image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import threading\n\n# Global model objects for single initialization (thread-safe)\n_global_vqa_evaluator = None\n_global_aesthetic_evaluator = None\n_evaluator_lock = threading.Lock()\n\ndef initialize_evaluators():\n    \"\"\"Thread-safe, initialize and cache evaluators.\"\"\"\n    global _global_vqa_evaluator, _global_aesthetic_evaluator\n    with _evaluator_lock:\n        if _global_vqa_evaluator is None:\n            print(\"[Evaluator] Initializing VQA Evaluator...\")\n            _global_vqa_evaluator = VQAEvaluator()\n        if _global_aesthetic_evaluator is None:\n            print(\"[Evaluator] Initializing Aesthetic Evaluator...\")\n            _global_aesthetic_evaluator = AestheticEvaluator()\n    return _global_vqa_evaluator, _global_aesthetic_evaluator\n\ndef evaluate_svg(svg: str, prompt: str, \n                 vqa_weight: float = 2.0, \n                 cache: dict = None, \n                 complexity_alpha: float = 0.015,\n                 verbose: bool = False) -> dict:\n    \"\"\"\n    Evaluate SVG illustration quality for a competition:\n      - VQA (semantic relevance to prompt)\n      - Aesthetics (CLIP-based)\n      - SVG complexity penalty (optional)\n    Returns a dict of scores.\n    \"\"\"\n    cache_key = (svg, prompt)\n    if cache is not None and cache_key in cache:\n        return cache[cache_key]\n\n    vqa_evaluator, aesthetic_evaluator = initialize_evaluators()\n    image = svg_to_png(svg)\n    \n    vqa_prompt = f\"SVG illustration of {prompt}\"\n    vqa_score = vqa_evaluator.score(image, vqa_prompt)\n    aesthetic_score = aesthetic_evaluator.score(image)\n    complexity_penalty = svg_complexity_penalty(svg, alpha=complexity_alpha)\n\n    combined_score = harmonic_mean(vqa_score, aesthetic_score, beta=vqa_weight) * complexity_penalty\n\n    result = {\n        'vqa_score': vqa_score,\n        'aesthetic_score': aesthetic_score,\n        'complexity_penalty': complexity_penalty,\n        'combined_score': combined_score\n    }\n    if cache is not None:\n        cache[cache_key] = result\n    if verbose:\n        print(f\"[Evaluator] Prompt: {prompt}\\n\"\n              f\"  VQA: {vqa_score:.3f}, Aesthetic: {aesthetic_score:.3f}, \"\n              f\"Complexity Penalty: {complexity_penalty:.3f}, \"\n              f\"Combined: {combined_score:.3f}\")\n    return result\n\ndef svg_complexity_penalty(svg_code: str, alpha: float = 0.015) -> float:\n    \"\"\"\n    Penalize SVGs with high command count to encourage simplicity.\n    \"\"\"\n    import re\n    num_commands = len(re.findall(r'([MLCQZmlcqz])', svg_code))\n    return float(np.exp(-alpha * num_commands))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"default_svgs = [\n    # Advanced geometric + abstract SVG\n    '''\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"384\" height=\"384\">\n      <rect width=\"100%\" height=\"100%\" fill=\"#f5f5f5\"/>\n      <g stroke=\"#262626\" stroke-width=\"2\">\n        <rect x=\"40\" y=\"40\" width=\"304\" height=\"304\" rx=\"32\" fill=\"#e0eafc\"/>\n        <circle cx=\"192\" cy=\"192\" r=\"108\" fill=\"#aad8d3\" stroke=\"#2e3440\" stroke-width=\"8\"/>\n        <polygon points=\"192,70 250,314 134,314\" fill=\"#ef476f\" opacity=\"0.85\" stroke=\"#2e3440\" stroke-width=\"2\"/>\n        <polyline points=\"96,160 288,160 192,320 96,160\" fill=\"none\" stroke=\"#0077b6\" stroke-width=\"6\" opacity=\"0.7\"/>\n        <path d=\"M130 110 Q192 40 254 110 T254 274 Q192 344 130 274 T130 110\" fill=\"none\" stroke=\"#ff006e\" stroke-width=\"4\" opacity=\"0.8\"/>\n      </g>\n      <g fill=\"none\" stroke-linecap=\"round\" stroke-linejoin=\"round\">\n        <path stroke=\"#FF4700\" stroke-width=\"13\" d=\"M160 220q140-10 55-93\"/>\n        <path stroke=\"#FF5E00\" stroke-width=\"6\" d=\"M88 83q-14 90 170 74\"/>\n        <path stroke=\"#FDA1F6\" stroke-opacity=\".5\" stroke-width=\"11\" d=\"M280 175q23 27-200 100\"/>\n        <path stroke=\"#C77145\" stroke-opacity=\".7\" stroke-width=\"19\" d=\"M133 256q132 0 120-87\"/>\n      </g>\n    </svg>\n    ''',\n\n    # Layered SVG with ellipses and gradients\n    '''\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"384\" height=\"384\">\n      <defs>\n        <radialGradient id=\"bg\" cx=\"50%\" cy=\"50%\" r=\"65%\">\n          <stop offset=\"0%\" stop-color=\"#fffbe7\"/>\n          <stop offset=\"100%\" stop-color=\"#dbeafe\"/>\n        </radialGradient>\n      </defs>\n      <rect width=\"384\" height=\"384\" fill=\"url(#bg)\"/>\n      <ellipse cx=\"192\" cy=\"192\" rx=\"140\" ry=\"80\" fill=\"#a3be8c\" fill-opacity=\"0.7\"/>\n      <ellipse cx=\"120\" cy=\"192\" rx=\"60\" ry=\"140\" fill=\"#5e81ac\" fill-opacity=\"0.6\"/>\n      <ellipse cx=\"264\" cy=\"192\" rx=\"60\" ry=\"140\" fill=\"#bf616a\" fill-opacity=\"0.5\"/>\n      <circle cx=\"192\" cy=\"192\" r=\"48\" fill=\"#ebcb8b\"/>\n      <g stroke=\"#495867\" stroke-width=\"4\" fill=\"none\">\n        <path d=\"M140 120 Q192 40 244 120\"/>\n        <path d=\"M244 264 Q192 344 140 264\"/>\n        <path d=\"M80 192 Q192 80 304 192\"/>\n        <path d=\"M304 192 Q192 304 80 192\"/>\n      </g>\n    </svg>\n    ''',\n\n    # Abstract paths and corners\n    '''\n    <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"384\" height=\"384\">\n      <rect width=\"384\" height=\"384\" fill=\"#fafafa\"/>\n      <rect x=\"0\" y=\"0\" width=\"384\" height=\"48\" fill=\"#b4c9e7\"/>\n      <rect x=\"0\" y=\"336\" width=\"384\" height=\"48\" fill=\"#b4c9e7\"/>\n      <rect x=\"0\" y=\"0\" width=\"48\" height=\"384\" fill=\"#b4c9e7\"/>\n      <rect x=\"336\" y=\"0\" width=\"48\" height=\"384\" fill=\"#b4c9e7\"/>\n      <g fill=\"none\" stroke=\"#4361ee\" stroke-width=\"10\" opacity=\"0.7\">\n        <path d=\"M96 96 Q192 32 288 96\"/>\n        <path d=\"M288 288 Q192 352 96 288\"/>\n        <polyline points=\"96,96 192,192 288,288\"/>\n        <polyline points=\"288,96 192,192 96,288\"/>\n      </g>\n      <g>\n        <circle cx=\"192\" cy=\"192\" r=\"40\" fill=\"#fcbf49\" stroke=\"#f77f00\" stroke-width=\"6\"/>\n      </g>\n    </svg>\n    ''',\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\ndef get_best_default_svg(description, verbose=True):\n    \"\"\"\n    Select the best scoring SVG from the default_svgs list for a given description.\n    Uses your competition metric to score each SVG.\n    Returns (svg_string, score).\n    \"\"\"\n    global default_svgs\n\n    best_svg = None\n    best_score = float('-inf')\n\n    for idx, svg in enumerate(default_svgs):\n        try:\n            scores = evaluate_with_competition_metric(svg, description)\n            combined_score = scores.get(\"combined_score\", 0)\n            if verbose:\n                print(f\"[Default SVG {idx+1}/{len(default_svgs)}] Score: {combined_score:.4f}\")\n        except Exception as e:\n            if verbose:\n                print(f\"[Default SVG {idx+1}/{len(default_svgs)}] Scoring error: {e}\")\n            continue\n\n        if combined_score > best_score:\n            best_svg = svg\n            best_score = combined_score\n\n    if verbose:\n        print(f\"Best default SVG score: {best_score:.4f}\")\n\n    return best_svg, best_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#| export\n\nimport time\nimport traceback\n\ndef generate_and_convert_advanced(\n    prompt_prefix=\"\",\n    description=\"\",\n    prompt_suffix=\"\",\n    negative_prompt=\"\",\n    num_attempts=3,\n    num_inference_steps=20,\n    guidance_scale=15,\n    verbose=True,\n    fallback_svg='<svg width=\"200\" height=\"200\"></svg>',\n    cache=None,\n    postprocess_svg=None,\n    save_attempt_hook=None,\n    display_images=True,\n    use_best_default=True\n):\n    \"\"\"\n    Advanced pipeline for generating, converting, and evaluating SVGs for competition:\n      - Attempts multiple generations, tracks and returns the best-scoring result.\n      - Optionally uses the best default SVG as a baseline.\n      - Supports caching, hooks, and optional postprocessing.\n      - Prints detailed diagnostics and timing.\n    \"\"\"\n    \n    prompt = prompt_prefix + \" \" + description + \" \" + prompt_suffix\n    total_start_time = time.time()\n    attempt_data = []\n\n    # 1. Baseline: Best default SVG\n    best_svg, best_score = None, float('-inf')\n    if use_best_default:\n        try:\n            default_svg, default_score = get_best_default_svg(description, verbose=verbose)\n            if verbose:\n                print(f\"\\n[Baseline] Best default SVG score: {default_score:.4f}\")\n            best_svg = default_svg\n            best_score = default_score\n            attempt_data.append({\n                'svg': default_svg,\n                'score': default_score,\n                'type': 'default'\n            })\n        except Exception as e:\n            if verbose:\n                print(\"[Baseline] Default SVG selection error:\", e)\n            best_svg = fallback_svg\n            best_score = 0.0\n\n    # 2. Generation Loop\n    timing = {\n        'generation': [],\n        'conversion': [],\n        'evaluation': [],\n        'postprocess': [],\n        'attempt': []\n    }\n    \n    for i in range(num_attempts):\n        if verbose: print(f\"\\n=== Generation Attempt {i+1}/{num_attempts} ===\")\n        attempt_start = time.time()\n\n        # --- Generation ---\n        try:\n            t0 = time.time()\n            bitmap = generate_bitmap(\n                prompt,\n                negative_prompt=negative_prompt,\n                num_inference_steps=num_inference_steps,\n                guidance_scale=guidance_scale\n            )\n            t1 = time.time()\n            timing['generation'].append(t1 - t0)\n            if verbose: print(f\"Bitmap generation: {t1-t0:.2f}s\")\n        except Exception as e:\n            if verbose:\n                print(f\"[Attempt {i+1}] Bitmap generation failed:\", e)\n                traceback.print_exc()\n            continue\n\n        # --- Conversion ---\n        try:\n            t0 = time.time()\n            svg_content = bitmap_to_svg_layered(bitmap)\n            t1 = time.time()\n            timing['conversion'].append(t1 - t0)\n            if verbose: print(f\"SVG conversion: {t1-t0:.2f}s (size {len(svg_content.encode('utf-8'))} bytes)\")\n        except Exception as e:\n            if verbose:\n                print(f\"[Attempt {i+1}] SVG conversion failed:\", e)\n                traceback.print_exc()\n            continue\n\n        # --- Optional Postprocessing ---\n        if postprocess_svg:\n            t0 = time.time()\n            try:\n                svg_content = postprocess_svg(svg_content)\n            except Exception as e:\n                if verbose:\n                    print(f\"[Attempt {i+1}] Postprocessing error:\", e)\n            t1 = time.time()\n            timing['postprocess'].append(t1 - t0)\n            if verbose: print(f\"SVG postprocessing: {t1-t0:.2f}s\")\n\n        # --- Render SVG for evaluation ---\n        try:\n            rendered_svg = svg_to_png(svg_content)\n        except Exception as e:\n            if verbose:\n                print(f\"[Attempt {i+1}] SVG rendering failed:\", e)\n                traceback.print_exc()\n            continue\n\n        # --- Display images side by side ---\n        if display_images or verbose:\n            try:\n                import matplotlib.pyplot as plt\n                plt.figure(figsize=(12, 6))\n                plt.subplot(1, 2, 1)\n                plt.imshow(bitmap)\n                plt.title(f\"Original Image {i+1}\")\n                plt.axis('off')\n                plt.subplot(1, 2, 2)\n                plt.imshow(rendered_svg)\n                plt.title(f\"SVG Conversion {i+1}\")\n                plt.axis('off')\n                plt.tight_layout()\n                plt.show()\n            except Exception as e:\n                if verbose:\n                    print(f\"[Attempt {i+1}] Display error:\", e)\n\n        # --- Evaluation (with optional caching) ---\n        cache_key = (svg_content, description)\n        svg_scores = None\n        t0 = time.time()\n        try:\n            if cache is not None and cache_key in cache:\n                svg_scores = cache[cache_key]\n                if verbose: print(\"Loaded score from cache.\")\n            else:\n                svg_scores = evaluate_with_competition_metric(svg_content, description)\n                if cache is not None:\n                    cache[cache_key] = svg_scores\n            t1 = time.time()\n            timing['evaluation'].append(t1 - t0)\n        except Exception as e:\n            if verbose:\n                print(f\"[Attempt {i+1}] SVG evaluation failed:\", e)\n                traceback.print_exc()\n            continue\n\n        # --- Diagnostics ---\n        if svg_scores and verbose:\n            print(f\"Scores: VQA={svg_scores.get('vqa_score',0):.4f}, \"\n                  f\"Aesthetic={svg_scores.get('aesthetic_score',0):.4f}, \"\n                  f\"Combined={svg_scores.get('combined_score',0):.4f}\")\n\n        # --- Save hook ---\n        if save_attempt_hook:\n            try:\n                save_attempt_hook(\n                    attempt_index=i+1,\n                    prompt=prompt,\n                    bitmap=bitmap,\n                    svg_content=svg_content,\n                    svg_scores=svg_scores\n                )\n            except Exception as e:\n                if verbose:\n                    print(f\"[Attempt {i+1}] Save hook error:\", e)\n\n        # --- Track best ---\n        current_score = svg_scores.get('combined_score', 0)\n        attempt_data.append({\n            'svg': svg_content,\n            'score': current_score,\n            'type': 'generated',\n            'scores': svg_scores,\n            'bitmap': bitmap\n        })\n        if current_score > best_score:\n            best_svg = svg_content\n            best_score = current_score\n            if verbose: print(f\"✅ New best result: {current_score:.4f}\")\n        else:\n            if verbose: print(f\"❌ Not better than current best: {best_score:.4f}\")\n\n        timing['attempt'].append(time.time() - attempt_start)\n\n    # --- Summary ---\n    total_time = time.time() - total_start_time\n    if verbose:\n        print(\"\\n=== Timing Summary ===\")\n        for k in ['generation', 'conversion', 'postprocess', 'evaluation', 'attempt']:\n            if timing[k]:\n                print(f\"Avg {k} time: {sum(timing[k])/len(timing[k]):.2f}s\")\n        print(f\"Total pipeline time: {total_time:.2f}s\")\n        print(f\"Best SVG score: {best_score:.4f}\")\n\n    return best_svg, best_score, attempt_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_svg, best_score, all_attempts = generate_and_convert_advanced(\n    prompt_prefix=\"Close up of\",\n    description=\"a lighthouse overlooking the ocean\",\n    prompt_suffix=\"flat color blocks, beautiful, minimal details, solid colors only\",\n    negative_prompt=\"lines, hatching, textures, patterns, details, outlines\",\n    num_inference_steps=15,\n    num_attempts=3,\n    verbose=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}