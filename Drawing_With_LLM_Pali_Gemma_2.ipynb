{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 89659,
          "databundleVersionId": 11735795,
          "sourceType": "competition"
        },
        {
          "sourceId": 224423433,
          "sourceType": "kernelVersion"
        },
        {
          "sourceId": 263093,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 225001,
          "modelId": 164716
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Drawing With LLM-Pali Gemma 2",
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "4bzjnKj_QwPN"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "drawing_with_llms_path = kagglehub.competition_download('drawing-with-llms')\n",
        "metric_svg_constraints_path = kagglehub.package_import('metric/svg-constraints')\n",
        "google_paligemma_2_transformers_paligemma2_10b_mix_448_1_path = kagglehub.model_download('google/paligemma-2/Transformers/paligemma2-10b-mix-448/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "kdsRjLtEQwPP"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppresses INFO and WARNING logs\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T06:09:49.576989Z",
          "iopub.execute_input": "2025-05-17T06:09:49.577446Z",
          "iopub.status.idle": "2025-05-17T06:09:49.58208Z",
          "shell.execute_reply.started": "2025-05-17T06:09:49.57742Z",
          "shell.execute_reply": "2025-05-17T06:09:49.581147Z"
        },
        "id": "xRPjx7vXQwPQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kaagle Metric\n"
      ],
      "metadata": {
        "id": "wvr51X0mQwPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import io\n",
        "import math\n",
        "import statistics\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageFilter\n",
        "import cairosvg\n",
        "import clip\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "from more_itertools import chunked\n",
        "\n",
        "import kagglehub\n",
        "svg_constraints = kagglehub.package_import('metric/svg-constraints')\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    pass\n",
        "\n",
        "def score(\n",
        "    solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, random_seed: int = 0\n",
        ") -> float:\n",
        "    for colname in ['question', 'choices', 'answer']:\n",
        "        solution[colname] = solution[colname].apply(ast.literal_eval)\n",
        "    solution = solution.explode(['question', 'choices', 'answer'])\n",
        "\n",
        "    if not pd.api.types.is_string_dtype(submission.loc[:, 'svg']):\n",
        "        raise ParticipantVisibleError('svg must be a string.')\n",
        "\n",
        "    constraints = svg_constraints.SVGConstraints()\n",
        "    try:\n",
        "        for svg in submission.loc[:, 'svg']:\n",
        "            constraints.validate_svg(svg)\n",
        "    except Exception:\n",
        "        raise ParticipantVisibleError('SVG code violates constraints.')\n",
        "\n",
        "    vqa_evaluator = VQAEvaluator()\n",
        "    aesthetic_evaluator = AestheticEvaluator()\n",
        "\n",
        "    results = []\n",
        "    rng = np.random.RandomState(random_seed)\n",
        "    try:\n",
        "        df = solution.merge(submission, on='id')\n",
        "        for i, (_, group) in enumerate(df.loc[\n",
        "            :, ['id', 'question', 'choices', 'answer', 'svg']\n",
        "        ].groupby('id')):\n",
        "            questions, choices, answers, svg = [\n",
        "                group[col_name].to_list()\n",
        "                for col_name in group.drop('id', axis=1).columns\n",
        "            ]\n",
        "            svg = svg[0]\n",
        "            group_seed = rng.randint(0, np.iinfo(np.int32).max)\n",
        "            image_processor = ImageProcessor(image=svg_to_png(svg), seed=group_seed).apply()\n",
        "            image = image_processor.image.copy()\n",
        "            aesthetic_score = aesthetic_evaluator.score(image)\n",
        "            vqa_score = vqa_evaluator.score(questions, choices, answers, image)\n",
        "            image_processor.reset().apply_random_crop_resize().apply_jpeg_compression(quality=90)\n",
        "            ocr_score = vqa_evaluator.ocr(image_processor.image)\n",
        "            instance_score = (\n",
        "                harmonic_mean(vqa_score, aesthetic_score, beta=0.5) * ocr_score\n",
        "            )\n",
        "            results.append(instance_score)\n",
        "    except Exception:\n",
        "        raise ParticipantVisibleError('SVG failed to score.')\n",
        "\n",
        "    fidelity = statistics.mean(results)\n",
        "    return float(fidelity)\n",
        "\n",
        "class VQAEvaluator:\n",
        "    def __init__(self):\n",
        "        from transformers import (\n",
        "            AutoProcessor,\n",
        "            BitsAndBytesConfig,\n",
        "            PaliGemmaForConditionalGeneration,\n",
        "        )\n",
        "        self.quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type='nf4',\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "        )\n",
        "        self.letters = string.ascii_uppercase\n",
        "        self.model_path = kagglehub.model_download(\n",
        "            'google/paligemma-2/transformers/paligemma2-10b-mix-448'\n",
        "        )\n",
        "        self.processor = AutoProcessor.from_pretrained(self.model_path)\n",
        "        self.model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
        "            self.model_path,\n",
        "            low_cpu_mem_usage=True,\n",
        "            quantization_config=self.quantization_config,\n",
        "        ).to('cuda:0')\n",
        "\n",
        "    def score(self, questions, choices, answers, image, n=4):\n",
        "        scores = []\n",
        "        batches = (chunked(qs, n) for qs in [questions, choices, answers])\n",
        "        for question_batch, choice_batch, answer_batch in zip(*batches, strict=True):\n",
        "            scores.extend(\n",
        "                self.score_batch(\n",
        "                    image,\n",
        "                    question_batch,\n",
        "                    choice_batch,\n",
        "                    answer_batch,\n",
        "                )\n",
        "            )\n",
        "        return statistics.mean(scores)\n",
        "\n",
        "    def score_batch(\n",
        "        self,\n",
        "        image: Image.Image,\n",
        "        questions: list[str],\n",
        "        choices_list: list[list[str]],\n",
        "        answers: list[str],\n",
        "    ) -> list[float]:\n",
        "        prompts = [\n",
        "            self.format_prompt(question, choices)\n",
        "            for question, choices in zip(questions, choices_list, strict=True)\n",
        "        ]\n",
        "        batched_choice_probabilities = self.get_choice_probability(\n",
        "            image, prompts, choices_list\n",
        "        )\n",
        "\n",
        "        scores = []\n",
        "        for i, _ in enumerate(questions):\n",
        "            choice_probabilities = batched_choice_probabilities[i]\n",
        "            answer = answers[i]\n",
        "            answer_probability = 0.0\n",
        "            for choice, prob in choice_probabilities.items():\n",
        "                if choice == answer:\n",
        "                    answer_probability = prob\n",
        "                    break\n",
        "            scores.append(answer_probability)\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def format_prompt(self, question: str, choices: list[str]) -> str:\n",
        "        prompt = f'<image>answer en Question: {question}\\nChoices:\\n'\n",
        "        for i, choice in enumerate(choices):\n",
        "            prompt += f'{self.letters[i]}. {choice}\\n'\n",
        "        return prompt\n",
        "\n",
        "    def mask_choices(self, logits, choices_list):\n",
        "        batch_size = logits.shape[0]\n",
        "        masked_logits = torch.full_like(logits, float('-inf'))\n",
        "\n",
        "        for batch_idx in range(batch_size):\n",
        "            choices = choices_list[batch_idx]\n",
        "            for i in range(len(choices)):\n",
        "                letter_token = self.letters[i]\n",
        "                first_token = self.processor.tokenizer.encode(\n",
        "                    letter_token, add_special_tokens=False\n",
        "                )[0]\n",
        "                first_token_with_space = self.processor.tokenizer.encode(\n",
        "                    ' ' + letter_token, add_special_tokens=False\n",
        "                )[0]\n",
        "\n",
        "                if isinstance(first_token, int):\n",
        "                    masked_logits[batch_idx, first_token] = logits[\n",
        "                        batch_idx, first_token\n",
        "                    ]\n",
        "                if isinstance(first_token_with_space, int):\n",
        "                    masked_logits[batch_idx, first_token_with_space] = logits[\n",
        "                        batch_idx, first_token_with_space\n",
        "                    ]\n",
        "\n",
        "        return masked_logits\n",
        "\n",
        "    def get_choice_probability(self, image, prompts, choices_list) -> list[dict]:\n",
        "        inputs = self.processor(\n",
        "            images=[image] * len(prompts),\n",
        "            text=prompts,\n",
        "            return_tensors='pt',\n",
        "            padding='longest',\n",
        "        ).to('cuda:0')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "            masked_logits = self.mask_choices(logits, choices_list)\n",
        "            probabilities = torch.softmax(masked_logits, dim=-1)\n",
        "\n",
        "        batched_choice_probabilities = []\n",
        "        for batch_idx in range(len(prompts)):\n",
        "            choice_probabilities = {}\n",
        "            choices = choices_list[batch_idx]\n",
        "            for i, choice in enumerate(choices):\n",
        "                letter_token = self.letters[i]\n",
        "                first_token = self.processor.tokenizer.encode(\n",
        "                    letter_token, add_special_tokens=False\n",
        "                )[0]\n",
        "                first_token_with_space = self.processor.tokenizer.encode(\n",
        "                    ' ' + letter_token, add_special_tokens=False\n",
        "                )[0]\n",
        "\n",
        "                prob = 0.0\n",
        "                if isinstance(first_token, int):\n",
        "                    prob += probabilities[batch_idx, first_token].item()\n",
        "                if isinstance(first_token_with_space, int):\n",
        "                    prob += probabilities[batch_idx, first_token_with_space].item()\n",
        "                choice_probabilities[choice] = prob\n",
        "\n",
        "            total_prob = sum(choice_probabilities.values())\n",
        "            if total_prob > 0:\n",
        "                renormalized_probabilities = {\n",
        "                    choice: prob / total_prob\n",
        "                    for choice, prob in choice_probabilities.items()\n",
        "                }\n",
        "            else:\n",
        "                renormalized_probabilities = choice_probabilities\n",
        "            batched_choice_probabilities.append(renormalized_probabilities)\n",
        "\n",
        "        return batched_choice_probabilities\n",
        "\n",
        "    def ocr(self, image, free_chars=4):\n",
        "        inputs = (\n",
        "            self.processor(\n",
        "                text='<image>ocr\\n',\n",
        "                images=image,\n",
        "                return_tensors='pt',\n",
        "            )\n",
        "            .to(torch.float16)\n",
        "            .to(self.model.device)\n",
        "        )\n",
        "        input_len = inputs['input_ids'].shape[-1]\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            outputs = self.model.generate(**inputs, max_new_tokens=32, do_sample=False)\n",
        "            outputs = outputs[0][input_len:]\n",
        "            decoded = self.processor.decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        num_char = len(decoded)\n",
        "        return min(1.0, math.exp(-num_char + free_chars))\n",
        "\n",
        "class AestheticPredictor(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(self.input_size, 1024),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(1024, 128),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64, 16),\n",
        "            nn.Linear(16, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "class AestheticEvaluator:\n",
        "    def __init__(self):\n",
        "        self.model_path = '/kaggle/input/sac-logos-ava1-l14-linearmse/sac+logos+ava1-l14-linearMSE.pth'\n",
        "        self.clip_model_path = '/kaggle/input/openai-clip-vit-large-patch14/ViT-L-14.pt'\n",
        "        self.predictor, self.clip_model, self.preprocessor = self.load()\n",
        "\n",
        "    def load(self):\n",
        "        state_dict = torch.load(self.model_path, weights_only=True, map_location='cuda:1')\n",
        "        predictor = AestheticPredictor(768)\n",
        "        predictor.load_state_dict(state_dict)\n",
        "        predictor.to('cuda:1')\n",
        "        predictor.eval()\n",
        "        clip_model, preprocessor = clip.load(self.clip_model_path, device='cuda:1')\n",
        "        return predictor, clip_model, preprocessor\n",
        "\n",
        "    def score(self, image: Image.Image) -> float:\n",
        "        image = self.preprocessor(image).unsqueeze(0).to('cuda:1')\n",
        "        with torch.no_grad():\n",
        "            image_features = self.clip_model.encode_image(image)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "            image_features = image_features.cpu().detach().numpy()\n",
        "        score = self.predictor(torch.from_numpy(image_features).to('cuda:1').float())\n",
        "        return score.item() / 10.0\n",
        "\n",
        "def harmonic_mean(a: float, b: float, beta: float = 1.0) -> float:\n",
        "    if a <= 0 or b <= 0:\n",
        "        return 0.0\n",
        "    return (1 + beta**2) * (a * b) / (beta**2 * a + b)\n",
        "\n",
        "def svg_to_png(svg_code: str, size: tuple = (384, 384)) -> Image.Image:\n",
        "    if 'viewBox' not in svg_code:\n",
        "        svg_code = svg_code.replace('<svg', f'<svg viewBox=\"0 0 {size[0]} {size[1]}\"')\n",
        "    png_data = cairosvg.svg2png(bytestring=svg_code.encode('utf-8'))\n",
        "    return Image.open(io.BytesIO(png_data)).convert('RGB').resize(size)\n",
        "\n",
        "class ImageProcessor:\n",
        "    def __init__(self, image: Image.Image, seed=None):\n",
        "        self.image = image\n",
        "        self.original_image = self.image.copy()\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.RandomState(seed)\n",
        "        else:\n",
        "            self.rng = np.random\n",
        "\n",
        "    def reset(self):\n",
        "        self.image = self.original_image.copy()\n",
        "        return self\n",
        "\n",
        "    def apply_median_filter(self, size=3):\n",
        "        self.image = self.image.filter(ImageFilter.MedianFilter(size=size))\n",
        "        return self\n",
        "\n",
        "    def apply_bilateral_filter(self, d=9, sigma_color=75, sigma_space=75):\n",
        "        img_array = np.asarray(self.image)\n",
        "        filtered = cv2.bilateralFilter(img_array, d, sigma_color, sigma_space)\n",
        "        self.image = Image.fromarray(filtered)\n",
        "        return self\n",
        "\n",
        "    def apply_fft_low_pass(self, cutoff_frequency=0.5):\n",
        "        img_array = np.array(self.image, dtype=np.float32)\n",
        "        result = np.zeros_like(img_array)\n",
        "        for i in range(3):\n",
        "            f = np.fft.fft2(img_array[:, :, i])\n",
        "            fshift = np.fft.fftshift(f)\n",
        "            rows, cols = img_array[:, :, i].shape\n",
        "            crow, ccol = rows // 2, cols // 2\n",
        "            mask = np.zeros((rows, cols), np.float32)\n",
        "            r = int(min(crow, ccol) * cutoff_frequency)\n",
        "            center = [crow, ccol]\n",
        "            x, y = np.ogrid[:rows, :cols]\n",
        "            mask_area = (x - center[0]) ** 2 + (y - center[1]) ** 2 <= r * r\n",
        "            mask[mask_area] = 1\n",
        "            fshift_filtered = fshift * mask\n",
        "            f_ishift = np.fft.ifftshift(fshift_filtered)\n",
        "            img_back = np.fft.ifft2(f_ishift)\n",
        "            img_back = np.real(img_back)\n",
        "            result[:, :, i] = img_back\n",
        "        result = np.clip(result, 0, 255).astype(np.uint8)\n",
        "        self.image = Image.fromarray(result)\n",
        "        return self\n",
        "\n",
        "    def apply_jpeg_compression(self, quality=85):\n",
        "        buffer = io.BytesIO()\n",
        "        self.image.save(buffer, format='JPEG', quality=quality)\n",
        "        buffer.seek(0)\n",
        "        self.image = Image.open(buffer)\n",
        "        return self\n",
        "\n",
        "    def apply_random_crop_resize(self, crop_percent=0.05):\n",
        "        width, height = self.image.size\n",
        "        crop_pixels_w = int(width * crop_percent)\n",
        "        crop_pixels_h = int(height * crop_percent)\n",
        "        left = self.rng.randint(0, crop_pixels_w + 1)\n",
        "        top = self.rng.randint(0, crop_pixels_h + 1)\n",
        "        right = width - self.rng.randint(0, crop_pixels_w + 1)\n",
        "        bottom = height - self.rng.randint(0, crop_pixels_h + 1)\n",
        "        self.image = self.image.crop((left, top, right, bottom))\n",
        "        self.image = self.image.resize((width, height), Image.BILINEAR)\n",
        "        return self\n",
        "\n",
        "    def apply(self):\n",
        "        return (\n",
        "            self.apply_random_crop_resize(crop_percent=0.03)\n",
        "            .apply_jpeg_compression(quality=95)\n",
        "            .apply_median_filter(size=9)\n",
        "            .apply_fft_low_pass(cutoff_frequency=0.5)\n",
        "            .apply_bilateral_filter(d=5, sigma_color=75, sigma_space=75)\n",
        "            .apply_jpeg_compression(quality=92)\n",
        "        )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-17T06:26:38.384168Z",
          "iopub.execute_input": "2025-05-17T06:26:38.384448Z",
          "iopub.status.idle": "2025-05-17T06:26:38.633189Z",
          "shell.execute_reply.started": "2025-05-17T06:26:38.384428Z",
          "shell.execute_reply": "2025-05-17T06:26:38.632129Z"
        },
        "id": "jQoXur2GQwPR"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}